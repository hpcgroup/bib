@inproceedings{elmo,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@article{coat-net,
  author    = {Zihang Dai and
               Hanxiao Liu and
               Quoc V. Le and
               Mingxing Tan},
  title     = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},
  journal   = {CoRR},
  volume    = {abs/2106.04803},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.04803},
  eprinttype = {arXiv},
  eprint    = {2106.04803},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-04803.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{mpi4py,
  author={Dalcin, Lisandro and Fang, Yao-Lung L.},
  journal={Computing in Science   Engineering}, 
  title={mpi4py: Status Update After 12 Years of Development}, 
  year={2021},
  volume={23},
  number={4},
  pages={47-54},
  doi={10.1109/MCSE.2021.3083216}}

@article{Rumelhart:1986backprop,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

@inproceedings{rannc,
  author    = {Masahiro Tanaka and
               Kenjiro Taura and
               Toshihiro Hanawa and
               Kentaro Torisawa},
  title     = {Automatic Graph Partitioning for Very Large-scale Deep Learning},
  booktitle = {35th {IEEE} International Parallel and Distributed Processing Symposium,
               {IPDPS} 2021, Portland, OR, USA, May 17-21, 2021},
  pages     = {1004--1013},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/IPDPS49936.2021.00109},
  doi       = {10.1109/IPDPS49936.2021.00109},
  timestamp = {Fri, 02 Jul 2021 14:10:40 +0200},
  biburl    = {https://dblp.org/rec/conf/ipps/TanakaTHT21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{the_pile,
  author    = {Leo Gao and
               Stella Biderman and
               Sid Black and
               Laurence Golding and
               Travis Hoppe and
               Charles Foster and
               Jason Phang and
               Horace He and
               Anish Thite and
               Noa Nabeshima and
               Shawn Presser and
               Connor Leahy},
  title     = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal   = {CoRR},
  volume    = {abs/2101.00027},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.00027},
  eprinttype = {arXiv},
  eprint    = {2101.00027},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zero_infinity,
  author    = {Samyam Rajbhandari and
               Olatunji Ruwase and
               Jeff Rasley and
               Shaden Smith and
               Yuxiong He},
  title     = {ZeRO-Infinity: Breaking the {GPU} Memory Wall for Extreme Scale Deep
               Learning},
  journal   = {CoRR},
  volume    = {abs/2104.07857},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.07857},
  archivePrefix = {arXiv},
  eprint    = {2104.07857},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-07857.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zero_offload,
  author    = {Jie Ren and
               Samyam Rajbhandari and
               Reza Yazdani Aminabadi and
               Olatunji Ruwase and
               Shuangyan Yang and
               Minjia Zhang and
               Dong Li and
               Yuxiong He},
  title     = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  journal   = {CoRR},
  volume    = {abs/2101.06840},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.06840},
  archivePrefix = {arXiv},
  eprint    = {2101.06840},
  timestamp = {Mon, 03 May 2021 16:42:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-06840.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{zero_3D,
  author = {Microsoft},
  title = {3D parallelism with MegatronLM and ZeRO Redundancy Optimizer},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM-v1.1.5-3D_parallelism}},
  commit = {1bee84f6eb75ed7e39e34601bfdd66d79cafe99a}
}

@article{megatronlm-2,
  author    = {Deepak Narayanan and
               Mohammad Shoeybi and
               Jared Casper and
               Patrick LeGresley and
               Mostofa Patwary and
               Vijay Korthikanti and
               Dmitri Vainbrand and
               Prethvi Kashinkunti and
               Julie Bernauer and
               Bryan Catanzaro and
               Amar Phanishayee and
               Matei Zaharia},
  title     = {Efficient Large-Scale Language Model Training on {GPU} Clusters},
  journal   = {CoRR},
  volume    = {abs/2104.04473},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.04473},
  archivePrefix = {arXiv},
  eprint    = {2104.04473},
  timestamp = {Tue, 13 Apr 2021 16:46:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-04473.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{snli-te-dataset,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}


@article{squad-qa-dataset,
  author    = {Pranav Rajpurkar and
               Robin Jia and
               Percy Liang},
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  journal   = {CoRR},
  volume    = {abs/1806.03822},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.03822},
  archivePrefix = {arXiv},
  eprint    = {1806.03822},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-03822.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{text-clf-transformer,
  author    = {Shervin Minaee and
               Nal Kalchbrenner and
               Erik Cambria and
               Narjes Nikzad and
               Meysam Chenaghlu and
               Jianfeng Gao},
  title     = {Deep Learning Based Text Classification: {A} Comprehensive Review},
  journal   = {CoRR},
  volume    = {abs/2004.03705},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.03705},
  archivePrefix = {arXiv},
  eprint    = {2004.03705},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03705.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt-3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  archivePrefix = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v37-ioffe15,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {
http://proceedings.mlr.press/v37/ioffe15.html
}
}

@inproceedings{micikevicius2018mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@inproceedings{distbelief,
title	= {Large Scale Distributed Deep Networks},
author	= {Jeffrey Dean and Greg S. Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Quoc V. Le and Mark Z. Mao and Marc’Aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Andrew Y. Ng},
year	= {2012},
booktitle	= {NIPS}
}

@article{MLPerf2020,
  author    = {Peter Mattson and
               Christine Cheng and
               Cody Coleman and
               Greg Diamos and
               Paulius Micikevicius and
               David A. Patterson and
               Hanlin Tang and
               Gu{-}Yeon Wei and
               Peter Bailis and
               Victor Bittorf and
               David Brooks and
               Dehao Chen and
               Debojyoti Dutta and
               Udit Gupta and
               Kim M. Hazelwood and
               Andrew Hock and
               Xinyuan Huang and
               Bill Jia and
               Daniel Kang and
               David Kanter and
               Naveen Kumar and
               Jeffery Liao and
               Guokai Ma and
               Deepak Narayanan and
               Tayo Oguntebi and
               Gennady Pekhimenko and
               Lillian Pentecost and
               Vijay Janapa Reddi and
               Taylor Robie and
               Tom St. John and
               Carole{-}Jean Wu and
               Lingjie Xu and
               Cliff Young and
               Matei Zaharia},
  title     = {MLPerf Training Benchmark},
  journal   = {CoRR},
  volume    = {abs/1910.01500},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01500},
  archivePrefix = {arXiv},
  eprint    = {1910.01500},
  timestamp = {Mon, 04 Nov 2019 08:16:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wikitext-103,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  archivePrefix = {arXiv},
  eprint    = {1609.07843},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SGDMomentum,
title = {On the momentum term in gradient descent learning algorithms},
journal = {Neural Networks},
volume = {12},
number = {1},
pages = {145-151},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(98)00116-6},
url = {https://www.sciencedirect.com/science/article/pii/S0893608098001166},
author = {Ning Qian},
keywords = {Momentum, Gradient descent learning algorithm, Damped harmonic oscillator, Critical damping, Learning rate, Speed of convergence},
abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.}
}

@article{weight_pred,
  title     = {Efficient and Robust Parallel {DNN} Training through Model Parallelism
               on Multi-GPU Platform},
  journal   = {CoRR},
  volume    = {abs/1809.02839},
  year      = {2018},
  note      = {Withdrawn.},
  url       = {http://arxiv.org/abs/1809.02839},
  archivePrefix = {arXiv},
  eprint    = {1809.02839},
  timestamp = {Fri, 04 Jan 2019 10:57:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-02839.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{amoebanet,
  author    = {Esteban Real and
               Alok Aggarwal and
               Yanping Huang and
               Quoc V. Le},
  title     = {Regularized Evolution for Image Classifier Architecture Search},
  journal   = {CoRR},
  volume    = {abs/1802.01548},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.01548},
  archivePrefix = {arXiv},
  eprint    = {1802.01548},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-01548.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{amoebanet-aaai, 
  title={Regularized Evolution for Image Classifier Architecture Search}, 
  volume={33}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/4405}, 
  DOI={10.1609/aaai.v33i01.33014780}, 
  abstractNote={&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;}, 
  number={01}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.}, 
  year={2019}, 
  month={Jul.}, 
  pages={4780-4789} 
}

@inproceedings{GEMS,
  author = {Jain, Arpan and Awan, Ammar Ahmad and Aljuhani, Asmaa M. and Hashmi, Jahanzeb Maqbool and Anthony, Quentin G. and Subramoni, Hari and Panda, Dhableswar K. and Machiraju, Raghu and Parwani, Anil},
  title = {GEMS: GPU-ENabled MEmory-Aware Model-Parallelism SYstem for Distributed DNN Training},
  year = {2020},
  isbn = {9781728199986},
  publisher = {IEEE Press},
  abstract = {Data-parallelism has become an established paradigm to train DNNs that fit inside GPU memory on large-scale HPC systems. However, model-parallelism is required to train out-of-core DNNs. In this paper, we deal with emerging requirements brought forward by very large DNNs being trained using high-resolution images common in digital pathology. To address these, we propose, design, and implement GEMS; a GPU-Enabled Memory-Aware Model-Parallelism System. We present several design schemes like GEMS-MAST, GEMS-MASTER, and GEMS-Hybrid that offer excellent speedups over state-of-the-art systems like Mesh-TensorFlow and FlexFlow. Furthermore, we combine model-parallelism and data-parallelism to train a 1000-layer ResNet-1k model using 1,024 Volta V100 GPUs with 97.32% scaling-efficiency. For the real-world histopathology whole-slide-image (WSI) of 100,000 x 100,000 pixels, we train custom ResNet-110-v2 on image tiles of size 1024 x 1024 and reduce the training time from seven hours to 28 minutes.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {45},
  numpages = {15},
  keywords = {MPI, TensorFlow, model parallelism, DNN, eager execution, keras},
  location = {Atlanta, Georgia},
  series = {SC '20}
}



@inproceedings {projectadam,
author = {Trishul Chilimbi and Yutaka Suzue and Johnson Apacible and Karthik Kalyanaraman},
title = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {571--582},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
publisher = {{USENIX} Association},
month = oct,
}

@misc{niu2011hogwild,
      title={HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}, 
      author={Feng Niu and Benjamin Recht and Christopher Re and Stephen J. Wright},
      year={2011},
      eprint={1106.5730},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@inproceedings{niu2011hogwild-nips,
  author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
  url = {https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},
  volume = {24},
  year = {2011}
}


@ARTICLE{MoDNN,
  author={X. {Chen} and D. Z. {Chen} and Y. {Han} and X. S. {Hu}},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={moDNN: Memory Optimal Deep Neural Network Training on Graphics Processing Units}, 
  year={2019},
  volume={30},
  number={3},
  pages={646-661},
  doi={10.1109/TPDS.2018.2866582}}

@inproceedings{vDNN,
author = {Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W.},
title = {VDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design},
year = {2016},
publisher = {IEEE Press},
abstract = {The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89%, OverFeat by 91%, and GoogLeNet by 95%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18% performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {18},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{KingmaAdam2014,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mesh_tf,
 author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Mesh-TensorFlow: Deep Learning for Supercomputers},
 url = {https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{abadi2016tensorflow,
title={TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}, 
author={Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year={2016},
eprint={1603.04467},
archivePrefix={arXiv},
primaryClass={cs.DC}
}

@inproceedings{tensorflowosdi2016,
  author = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  year = {2016},
  isbn = {9781931971331},
  publisher = {USENIX Association},
  address = {USA},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  pages = {265–283},
  numpages = {19},
  location = {Savannah, GA, USA},
  series = {OSDI'16}
}


@misc{sergeev2018horovod,
      title={Horovod: fast and easy distributed deep learning in TensorFlow}, 
      author={Alexander Sergeev and Mike Del Balso},
      year={2018},
      eprint={1802.05799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kim2020torchgpipe,
    title={torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models},
    author={Chiheon Kim and Heungsub Lee and Myungryong Jeong and Woonhyuk Baek and Boogeon Yoon and Ildoo Kim and Sungbin Lim and Sungwoong Kim},
    year={2020},
    eprint={2004.09910},
    archivePrefix={arXiv}
}

@misc{you2018imagenet,
      title={ImageNet Training in Minutes}, 
      author={Yang You and Zhao Zhang and Cho-Jui Hsieh and James Demmel and Kurt Keutzer},
      year={2018},
      eprint={1709.05011},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{you2018imagenet-icpp,
  author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  title = {ImageNet Training in Minutes},
  year = {2018},
  isbn = {9781450365109},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3225058.3225069},
  doi = {10.1145/3225058.3225069},
  abstract = {In this paper, we investigate large scale computers' capability of speeding up deep neural networks (DNN) training. Our approach is to use large batch size, powered by the Layer-wise Adaptive Rate Scaling (LARS) algorithm, for efficient usage of massive computing resources. Our approach is generic, as we empirically evaluate the effectiveness on two neural networks: AlexNet and ResNet-50 trained with the ImageNet-1k dataset while preserving the state-of-the-art test accuracy. Compared to the baseline of a previous study from a group of researchers at Facebook, our approach shows higher test accuracy on batch sizes that are larger than 16K. Using 2,048 Intel Xeon Platinum 8160 processors, we reduce the 100-epoch AlexNet training time from hours to 11 minutes. With 2,048 Intel Xeon Phi 7250 Processors, we reduce the 90-epoch ResNet-50 training time from hours to 20 minutes. Our implementation is open source and has been released in the Intel distribution of Caffe v1.0.7.},
  booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
  articleno = {1},
  numpages = {10},
  keywords = {Fast Deep Neural Networks Training, Distributed Machine Learning},
  location = {Eugene, OR, USA},
  series = {ICPP 2018}
}

@misc{you2017large,
      title={Large Batch Training of Convolutional Networks}, 
      author={Yang You and Igor Gitman and Boris Ginsburg},
      year={2017},
      eprint={1708.03888},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jia2018data-arxiv,
      title={Beyond Data and Model Parallelism for Deep Neural Networks}, 
      author={Zhihao Jia and Matei Zaharia and Alex Aiken},
      year={2018},
      eprint={1807.05358},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{chen2016revisiting,
title	= {Revisiting Distributed Synchronous SGD},
author	= {Jianmin Chen and Rajat Monga and Samy Bengio and Rafal Jozefowicz},
year	= {2016},
URL	= {https://arxiv.org/abs/1604.00981},
booktitle	= {International Conference on Learning Representations Workshop Track}
}

@misc{chen2016training,
      title={Training Deep Nets with Sublinear Memory Cost}, 
      author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
      year={2016},
      eprint={1604.06174},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{wahib2020scaling,
    title={Scaling Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA},
    author={Mohamed Wahib and Haoyu Zhang and Truong Thao Nguyen and Aleksandr Drozd and Jens Domke and Lingqi Zhang and Ryousei Takano and Satoshi Matsuoka},
    year={2020},
    eprint={2008.11421},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@inproceedings{wahib2020scaling-sc,
  author = {Wahib, Mohamed and Zhang, Haoyu and Nguyen, Truong Thao and Drozd, Aleksandr and Domke, Jens and Zhang, Lingqi and Takano, Ryousei and Matsuoka, Satoshi},
  title = {Scaling Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA},
  year = {2020},
  isbn = {9781728199986},
  publisher = {IEEE Press},
  abstract = {The dedicated memory of hardware accelerators can be insufficient to store all weights and/or intermediate states of large deep learning models. Although model parallelism is a viable approach to reduce the memory pressure issue, significant modification of the source code and considerations for algorithms are required. An alternative solution is to use out-of-core methods instead of, or in addition to, data parallelism.We propose a performance model based on the concurrency analysis of out-of-core training behavior, and derive a strategy that combines layer swapping and redundant recomputing. We achieve an average of 1.52x speedup in six different models over the state-of-the-art out-of-core methods. We also introduce the first method to solve the challenging problem of out-of-core multi-node training by carefully pipelining gradient exchanges and performing the parameter updates on the host. Our data parallel out-of-core solution can outperform complex hybrid model parallelism in training large models, e.g. Megatron-LM and Turning-NLG.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {19},
  numpages = {15},
  keywords = {deep neural networks, out-of-core, GPUs},
  location = {Atlanta, Georgia},
  series = {SC '20}
}

@misc{you2019large,
    title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes },
    author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
    year={2019},
    eprint={1904.00962},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ying2018image,
    title={Image Classification at Supercomputer Scale},
    author={Chris Ying and Sameer Kumar and Dehao Chen and Tao Wang and Youlong Cheng},
    year={2018},
    eprint={1811.06992},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{summit2019,
  author={J. {Yin} and S. {Gahlot} and N. {Laanait} and K. {Maheshwari} and J. {Morrison} and S. {Dash} and M. {Shankar}},
  booktitle={2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS)}, 
  title={Strategies to Deploy and Scale Deep Learning on the Summit Supercomputer}, 
  year={2019},
  volume={},
  number={},
  pages={84-94},
  doi={10.1109/DLS49591.2019.00016}}

  @misc{oyama2020case,
      title={The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs with Hybrid Parallelism}, 
      author={Yosuke Oyama and Naoya Maruyama and Nikoli Dryden and Erin McCarthy and Peter Harrington and Jan Balewski and Satoshi Matsuoka and Peter Nugent and Brian Van Essen},
      year={2020},
      eprint={2007.12856},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{flexflow,
    title={Beyond Data and Model Parallelism for Deep Neural Networks},
    author={Zhihao Jia and Matei Zaharia and Alex Aiken},
    year={2018},
    eprint={1807.05358},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@inproceedings{jia2018data,
	author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
	booktitle = {Proceedings of Machine Learning and Systems},
	editor = {A. Talwalkar and V. Smith and M. Zaharia},
	pages = {1--13},
	title = {Beyond Data and Model Parallelism for Deep Neural Networks.},
	url = {https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
	volume = {1},
	year = {2019}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v37-schulman15,
title = 	 {Trust Region Policy Optimization},
author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
pages = 	 {1889--1897},
year = 	 {2015},
editor = 	 {Francis Bach and David Blei},
volume = 	 {37},
series = 	 {Proceedings of Machine Learning Research},
address = 	 {Lille, France},
month = 	 {07--09 Jul},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
url = 	 {
http://proceedings.mlr.press/v37/schulman15.html
},

}

@misc{mnih2013playing,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Purwins_2019,
   title={Deep Learning for Audio Signal Processing},
   volume={13},
   ISSN={1941-0484},
   url={http://dx.doi.org/10.1109/JSTSP.2019.2908700},
   DOI={10.1109/jstsp.2019.2908700},
   number={2},
   journal={IEEE Journal of Selected Topics in Signal Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schluter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
   year={2019},
   month={May},
   pages={206–219}
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{paszke2019pytorch-nips,
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume = {32},
  year = {2019}
}

@misc{sinha2017introspection,
      title={Introspection: Accelerating Neural Network Training By Learning Weight Evolution}, 
      author={Abhishek Sinha and Mausoom Sarkar and Aahitagni Mukherjee and Balaji Krishnamurthy},
      year={2017},
      eprint={1704.04959},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2019efficient,
      title={Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform}, 
      author={Chi-Chung Chen and Chia-Lin Yang and Hsiang-Yun Cheng},
      year={2019},
      eprint={1809.02839},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{mirhoseini2017device,
      title={Device Placement Optimization with Reinforcement Learning}, 
      author={Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy Bengio and Jeff Dean},
      year={2017},
      eprint={1706.04972},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{Rosenblatt58theperceptron,
    author = {F. Rosenblatt},
    title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
    journal = {Psychological Review},
    year = {1958},
    pages = {65--386}
}

@misc{nikolentzos2019message,
      title={Message Passing Attention Networks for Document Understanding}, 
      author={Giannis Nikolentzos and Antoine J. -P. Tixier and Michalis Vazirgiannis},
      year={2019},
      eprint={1908.06267},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{nikolentzos2019message-aaai, 
  title={Message Passing Attention Networks for Document Understanding}, 
  volume={34}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/6376}, 
  DOI={10.1609/aaai.v34i05.6376}, 
  abstractNote={&lt;p&gt;Graph neural networks have recently emerged as a very effective framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks. Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose several hierarchical variants of MPAD. Experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the different components on performance. Code is publicly available at: https://github.com/giannisnik/mpad.&lt;/p&gt;}, 
  number={05}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Nikolentzos, Giannis and Tixier, Antoine and Vazirgiannis, Michalis}, 
  year={2020}, 
  month={Apr.}, 
  pages={8544-8551} 
}

@article{nmt,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}



@misc{vijayanarasimhan2017sfmnet,
      title={SfM-Net: Learning of Structure and Motion from Video}, 
      author={Sudheendra Vijayanarasimhan and Susanna Ricco and Cordelia Schmid and Rahul Sukthankar and Katerina Fragkiadaki},
      year={2017},
      eprint={1704.07804},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhao2019object,
      title={Object Detection with Deep Learning: A Review}, 
      author={Zhong-Qiu Zhao and Peng Zheng and Shou-tao Xu and Xindong Wu},
      year={2019},
      eprint={1807.05511},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{zhao2019object-tnnls,  
  author={Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},  
  journal={IEEE Transactions on Neural Networks and Learning Systems},   
  title={Object Detection With Deep Learning: A Review},   
  year={2019},  
  volume={30},  
  number={11},  
  pages={3212-3232},  
  doi={10.1109/TNNLS.2018.2876865}
}

@misc{tao2020hierarchical,
      title={Hierarchical Multi-Scale Attention for Semantic Segmentation}, 
      author={Andrew Tao and Karan Sapra and Bryan Catanzaro},
      year={2020},
      eprint={2005.10821},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{dryden2019,
author = {Dryden, Nikoli and Maruyama, Naoya and Moon, Tim and Benson, Tom and Snir, Marc and Van Essen, Brian},
title = {Channel and Filter Parallelism for Large-Scale CNN Training},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356207},
doi = {10.1145/3295500.3356207},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {10},
numpages = {20},
keywords = {convolution, CNN, deep learning, algorithms, scaling},
location = {Denver, Colorado},
series = {SC '19}
}


@article{BiT-resnet,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-11370.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{BiT-resnet-eccv,
  author="Kolesnikov, Alexander
  and Beyer, Lucas
  and Zhai, Xiaohua
  and Puigcerver, Joan
  and Yung, Jessica
  and Gelly, Sylvain
  and Houlsby, Neil",
  editor="Vedaldi, Andrea
  and Bischof, Horst
  and Brox, Thomas
  and Frahm, Jan-Michael",
  title="Big Transfer (BiT): General Visual Representation Learning",
  booktitle="Computer Vision -- ECCV 2020",
  year="2020",
  publisher="Springer International Publishing",
  address="Cham",
  pages="491--507",
  abstract="Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes---from 1 example per class to 1M total examples. BiT achieves 87.5{\%} top-1 accuracy on ILSVRC-2012, 99.4{\%} on CIFAR-10, and 76.3{\%} on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8{\%} on ILSVRC-2012 with 10 examples per class, and 97.0{\%} on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.",
  isbn="978-3-030-58558-7"
}

@article{yanyou-large-batch,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  archivePrefix = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalDGNWKTJH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{nccl,
  author = {NVIDIA},
  title = {NCCL},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html}},
  commit = {}
}

@misc{deepspeed-extreme-3d,
  author = {Microsoft},
  title = {Deepspeed: Extreme-scale model training for everyone},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/}},
  commit = {}
}

@misc{gpuDirect,
  author = {NVIDIA},
  title = {GPUDirect},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://docs.nvidia.com/cuda/gpudirect-rdma/index.html}},
  commit = {}
}

@misc{nsys,
  author = {NVIDIA},
  title = {NVIDIA Nsight Systems},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://developer.nvidia.com/nsight-systems}},
  commit = {}
}

@misc{osu-5.8,
  author = {Ohio State University},
  title = {OSU Micro-Benchmarks 5.8},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{http://mvapich.cse.ohio-state.edu/benchmarks/}},
  commit = {}
}

@misc{nvtx,
  author = {NVIDIA},
  title = {NVIDIA Nsight Systems},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://github.com/NVIDIA/NVTX}},
  commit = {}
}

@misc{nvlink,
  author = {NVIDIA},
  title = {NVLink and NVSwitch: The Building Blocks of Advanced Multi-GPU Communication},
  year = {},
  publisher = {},
  journal = {},
  howpublished = {\url{https://www.nvidia.com/en-us/data-center/nvlink/}},
  commit = {}
}

@misc{gloo,
  author = {Facebook},
  title = {Gloo : Collective communications library with various primitives for multi-machine training },
  year = {},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/facebookincubator/gloo}},
  commit = {66a5b347f6610cb057d09d64dfdb6984508a8c69}
}

@misc{rhu2016vdnn,
      title={vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design}, 
      author={Minsoo Rhu and Natalia Gimelshein and Jason Clemons and Arslan Zulfiqar and Stephen W. Keckler},
      year={2016},
      eprint={1602.08124},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{huang2019gpipe,
      title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}, 
      author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
      year={2019},
      eprint={1811.06965},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{huang2019gpipe_nips,
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, zhifeng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  url = {https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf},
  volume = {32},
  year = {2019}
}

@article{transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap
      between Human and Machine Translation},
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le
      and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan
      Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah
      and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws
      and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens
      and George Kurian and Nishant Patil and Wei Wang and Cliff Young and
      Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg
      Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wait-free-backprop,
  author    = {Hao Zhang and
               Zeyu Zheng and
               Shizhen Xu and
               Wei Dai and
               Qirong Ho and
               Xiaodan Liang and
               Zhiting Hu and
               Jinliang Wei and
               Pengtao Xie and
               Eric P. Xing},
  title     = {Poseidon: An Efficient Communication Architecture for Distributed
               Deep Learning on {GPU} Clusters},
  journal   = {CoRR},
  volume    = {abs/1706.03292},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03292},
  archivePrefix = {arXiv},
  eprint    = {1706.03292},
  timestamp = {Wed, 12 Dec 2018 16:25:50 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ZhangZXDHLHWXX17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wait-free-backprop-usenix,
  author = {Hao Zhang and Zeyu Zheng and Shizhen Xu and Wei Dai and Qirong Ho and Xiaodan Liang and Zhiting Hu and Jinliang Wei and Pengtao Xie and Eric P. Xing},
  title = {Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on {GPU} Clusters},
  booktitle = {2017 {USENIX} Annual Technical Conference ({USENIX} {ATC} 17)},
  year = {2017},
  isbn = {978-1-931971-38-6},
  address = {Santa Clara, CA},
  pages = {181--193},
  url = {https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang},
  publisher = {{USENIX} Association},
  month = jul,
}


@article{pytorchdist-vldb,
  author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
  title = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
  year = {2020},
  issue_date = {August 2020},
  publisher = {VLDB Endowment},
  volume = {13},
  number = {12},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3415478.3415530},
  doi = {10.14778/3415478.3415530},
  abstract = {This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. Py-Torch is a widely-adopted scientific computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.},
  journal = {Proc. VLDB Endow.},
  month = aug,
  pages = {3005–3018},
  numpages = {14}
}

@misc{megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{pmlr-v28-coates13, 
title = {Deep learning with COTS HPC systems}, 
author = {Adam Coates and Brody Huval and Tao Wang and David Wu and Bryan Catanzaro and Ng Andrew}, 
pages = {1337--1345}, 
year = {2013}, 
editor = {Sanjoy Dasgupta and David McAllester}, 
volume = {28}, 
number = {3}, 
series = {Proceedings of Machine Learning Research}, 
address = {Atlanta, Georgia, USA}, 
month = {17--19 Jun}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v28/coates13.pdf}, 
url = {http://proceedings.mlr.press/v28/coates13.html}, 
}

@misc{ericson2017performance,
      title={On the Performance of Network Parallel Training in Artificial
      Neural Networks},
      author={Ludvig Ericson and Rendani Mbuvha},
      year={2017},
      eprint={1701.05130},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@InProceedings{narayanan2019pipedream,
author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and
Seshadri, Vivek and Devanur, Nikhil and Granger, Greg and Gibbons, Phil and
Zaharia, Matei},
title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
booktitle = {ACM Symposium on Operating Systems Principles (SOSP 2019)},
year = {2019},
month = {October},
url =
{https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/},
}

@inproceedings{pipemare,
 author = {Yang, Bowen and Zhang, Jian and Li, Jonathan  and Re, Christopher and Aberger, Christopher and De Sa, Christopher},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {A. Smola and A. Dimakis and I. Stoica},
 pages = {269--296},
 title = {PipeMare: Asynchronous Pipeline Parallel DNN Training},
 url = {https://proceedings.mlsys.org/paper/2021/file/6c8349cc7260ae62e3b1396831a8398f-Paper.pdf},
 volume = {3},
 year = {2021}
}

@misc{vaswaniTransformer,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{MSRzero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sc2020zero,
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title = {ZeRO: Memory Optimizations toward Training Trillion Parameter Models},
  year = {2020},
  isbn = {9781728199986},
  publisher = {IEEE Press},
  abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {20},
  numpages = {16},
  location = {Atlanta, Georgia},
  series = {SC '20}
}

@misc{resnet,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{resnetCVPR,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
  title={Deep Residual Learning for Image Recognition},   
  year={2016},  
  volume={},  
  number={},  
  pages={770-778},  
  doi={10.1109/CVPR.2016.90}
}

@misc{vgg16,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{vgg16-iclr,
  author    = {Karen Simonyan and
                Andrew Zisserman},
  editor    = {Yoshua Bengio and
                Yann LeCun},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
                San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.1556},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt-2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@article{mnistlecun2010,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@inproceedings{lenet,
 author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {396--404},
 publisher = {Morgan-Kaufmann},
 title = {Handwritten Digit Recognition with a Back-Propagation Network},
 url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
 volume = {2},
 year = {1990}
}

@article{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}

@inproceedings{lockwood:sc2018,
  author = {Lockwood, Glenn K. and Snyder, Shane and Wang, Teng and Byna, Suren and Carns, Philip and Wright, Nicholas J.},
  title = {A Year in the Life of a Parallel File System},
  year = {2018},
  publisher = {IEEE Press},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
  articleno = {Article 74},
  numpages = {13},
  location = {Dallas, Texas},
  series = {SC ’18}
}

@article{kunen2015kripke,
  Author = {Kunen, AJ and Bailey, TS and Brown, PN},
  Journal = {Lawrence Livermore National Laboratory {(LLNL)}, Livermore, CA, Tech. Rep},
  Title = {{KRIPKE}-A massively parallel transport mini-app},
  Year = {2015}
}

@incollection{hypre,
  Author = {R.D. Falgout and J.E. Jones and U.M. Yang},
  Booktitle = {Numerical Solution of Partial Differential Equations on Parallel Computers},
  Editor = {A.M. Bruaset and A. Tveito},
  Pages = {267-294},
  Publisher = {Springer-Verlag},
  Title = {The Design and Implementation of hypre, a Library of Parallel High Performance Preconditioners},
  Volume = 51,
  Year = 2006
}

@techreport{RAJA,
  Author = {Hornung, Rich D. and Keasler, Jeff A.},
  Institution = {Lawrence Livermore National Laboratory},
  Month = sep,
  Number = {LLNL-TR-661403},
  Title = {{The RAJA Portability Layer: Overview and Status}},
  Year = {2014}
}

@InProceedings{jain:springer2016,
  author="Jain, Nikhil and Bohm, Eric and Mikida, Eric and Mandal, Subhasish and Kim, Minjung and Jindal, Prateek and Li, Qi and Ismail-Beigi, Sohrab and Martyna, Glenn J. and Kale, Laxmikant V.",
  editor="Kunkel, Julian M. and Balaji, Pavan and Dongarra, Jack",
  title="OpenAtom: Scalable Ab-Initio Molecular Dynamics with Diverse Capabilities",
  booktitle="High Performance Computing",
  year="2016",
  publisher="Springer International Publishing",
  address="Cham",
  pages="139--158",
}

@article{settles2012active,
  title={Active learning},
  author={Settles, Burr},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={6},
  number={1},
  pages={1--114},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@article{sukhija:2014,
  Author = {Sukhija, Nitin and Malone, Brandon and Srivastava, Srishti and Banicescu, Ioana and Ciorba, Florina M.},
  Journal = {Parallel and Cloud Computing},
  Month = oct,
  Number = {4},
  Pages = {66--81},
  Title = {A Learning-based Selection for Portfolio Scheduling of Scientific Applications on Heterogeneous Computing Systems},
  Volume = {3},
  Year = {2014}
}

@inproceedings{tallent:2015,
  Author = {Tallent, Nathan R. and Vishnu, Abhinav and Van Dam, Hubertus and Daily, Jeff and Kerbyson, Darren J. and Hoisie, Adolfy},
  Booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  Pages = {130--139},
  Publisher = {ACM},
  Series = {PPoPP 2015},
  Title = {Diagnosing the Causes and Severity of One-sided Message Contention},
  Url = {http://doi.acm.org/10.1145/2688500.2688516},
  Year = {2015}
}

@article{randomforests,
  Author = {Leo Breiman},
  Journal = {Machine Learning},
  Number = {1},
  Pages = {5-32},
  Title = {Random Forests},
  Volume = {45},
  Year = {2001}
}

@inproceedings{Fang:ICPP2011,
  Author = {J. Fang and A. L. Varbanescu and H. Sips},
  Booktitle = {2011 International Conference on Parallel Processing},
  Doi = {10.1109/ICPP.2011.45},
  Month = {Sept},
  Pages = {216-225},
  Title = {A Comprehensive Performance Comparison of CUDA and OpenCL},
  Year = {2011},
}

@article{ClintWhaley:2001cl,
  Author = {Clint Whaley, R and Petitet, Antoine and Dongarra, Jack J},
  Journal = {Parallel Computing},
  Month = jan,
  Number = {1-2},
  Pages = {3--35},
  Title = {{Automated empirical optimizations of software and the ATLAS project}},
  Volume = {27},
  Year = {2001}
}

@article{FFTW3,
  Author = {Frigo, M and Johnson, S G},
  Journal = {Proceedings of the IEEE},
  Number = {2},
  Pages = {216--231},
  Title = {{The Design and Implementation of FFTW3}},
  Volume = {93},
  Year = {2005}
}

@article{OSKI,
  Author = {Vuduc, Richard and Demmel, James W and Yelick, Katherine A},
  Journal = {Journal of Physics: Conference Series},
  Month = aug,
  Number = {1},
  Pages = {521--530},
  Title = {{OSKI: A library of automatically tuned sparse matrix kernels}},
  Volume = {16},
  Year = {2005}
}

@inproceedings{Ansel:2014gj,
  Address = {New York, New York, USA},
  Author = {Ansel, Jason and others},
  Booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation Techniques},
  Pages = {303--316},
  Title = {{OpenTuner}},
  Year = {2014}
}

@inproceedings{Orio,
  Author = {Hartono, Albert and Norris, Boyana and Sadayappan, P},
  Booktitle = {IEEE International Symposium on Parallel {\&} Distributed Processing},
  Month = may,
  Pages = {1--11},
  Title = {{Annotation-based empirical performance tuning using Orio}},
  Year = {2009}
}

@inproceedings{hollings:2002wg,
  Author = {Tapus, C and Chung, I-Hsin and Hollingsworth, J K},
  Booktitle = {Supercomputing 2002 (SC'02)},
  Month = nov,
  Pages = {44},
  Title = {{Active Harmony: Towards Automated Performance Tuning}},
  Year = {2002}
}

@inproceedings{hollings:1998,
  Author = {Hollingsworth, J K and Keleher, P J},
  Booktitle = {Proceedings of the 7th International Symposium on High Performance Distributed Computing},
  Month = jul,
  Pages = {180--188},
  Title = {{Prediction and adaptation in Active Harmony}},
  Year = {1998}
}

@inproceedings{Calotoiu:2013ewa,
  Author = {Calotoiu, Alexandru and Hoefler, Torsten and Poke, Marius and Wolf, Felix},
  Booktitle = {Supercomputing 2013 (SC'13)},
  Month = nov,
  Pages = {1--12},
  Title = {{Using automated performance modeling to find scalability bugs in complex codes}},
  Year = {2013}
}

@inproceedings{Bhattacharyya:2014fr,
  Address = {New York, New York, USA},
  Author = {Bhattacharyya, Arnamoy and Hoefler, Torsten},
  Booktitle = {The 23rd International Conference on Parallel Architecuters and Compilation Techniques},
  Pages = {393--404},
  Title = {{PEMOGEN}},
  Year = {2014}
}

@inproceedings{Hoefler:SC11,
  Author = {Hoefler, T and Gropp, W and Kramer, W and Snir, M},
  Booktitle = {Supercomputing 2011 (SC'11)},
  Pages = {1--12},
  Title = {{Performance modeling for systematic performance tuning}},
  Year = {2011}
}

@inproceedings{Bergstra:2012iz,
  Author = {Bergstra, J and Pinto, N and Cox, D},
  Booktitle = {Proceedings of Innovative Parallel Computing},
  Month = may,
  Pages = {1--9},
  Title = {{Machine learning for predictive auto-tuning with boosted regression trees}},
  Year = {2012}
}

@inproceedings{Didona:2015il,
  Address = {New York, New York, USA},
  Author = {Didona, Diego and Romano, Paolo},
  Booktitle = {the 6th ACM/SPEC International Conference},
  Pages = {341--344},
  Title = {{Hybrid Machine Learning/Analytical Models for Performance Prediction}},
  Year = {2015}
}

@inproceedings{Agakov:CGO2006,
  Author = {Agakov, F and others},
  Booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
  Month = mar,
  Pages = {295--305},
  Title = {{Using Machine Learning to Focus Iterative Optimization}},
  Year = {2006}
}

@inproceedings{Wu:HPCA2015,
  Author = {Wu, Gene and Greathouse, Joseph L and Lyashevsky, Alexander and Jayasena, Nuwan and Chiou, Derek},
  Booktitle = {Proceedings of the 21st IEEE International Symposium on High Performance Computer Architecture},
  Month = feb,
  Pages = {564--576},
  Title = {{GPGPU performance and power estimation using machine learning}},
  Year = {2015}
}

@inproceedings{Song:2013df,
  Author = {Song, S and Su, C and Rountree, B},
  Booktitle = {Proceedings of the 27th IEEE International Symposium on Parallel {\&} Distributed Processing},
  Month = may,
  Pages = {673--686},
  Title = {{A simplified and accurate model of power-performance efficiency on emergent GPU architectures}},
  Year = {2013}
}

@inproceedings{MatthewCurtisMaury:2007wi,
  Author = {Curtis-Maury, Matthew A and others},
  Booktitle = {Proceedings of the IEEE Conference on Cluster Computing},
  Month = sep,
  Pages = {488--495},
  Title = {{Identifying energy-efficient concurrency levels using machine learning}},
  Year = {2007}
}

@inproceedings{Su:wc2012,
  Author = {Su, Chunyi and Li, Dong and Nikolopoulos, Dimitrios S and Cameron, Kirk W and de Supinski, Bronis R and Leon, Edgar A},
  Booktitle = {Proceedings of the IEEE International Symposium on Workload Characterization},
  Month = nov,
  Pages = {164--173},
  Title = {{Model-based, memory-centric performance and power optimization on NUMA multiprocessors}},
  Year = {2012}
}

@inproceedings{beckingsale:ipdps2017,
  Author = {D. Beckingsale and O. Pearce and I. Laguna and T. Gamblin},
  Booktitle = {IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  Month = {May},
  Pages = {307-316},
  Title = {Apollo: Reusable Models for Fast, Dynamic Tuning of Input-Dependent Code},
  Year = {2017}
}

@inproceedings{yoo2011automated,
  Author = {Yoo, W. and Larson, K. and Kim, S. and Ahn, W. and Campbell, R and Baugh, L.},
  Booktitle = {Proc. of USENIX Workshop on Hot topics in parallelism. USENIX Association},
  Title = {Automated Fingerprinting of Performance Pathologies using Performance Monitoring Units (PMUs)},
  Year = {2011}
}

@article{Bhowmick06,
  Author = {Sanjukta Bhowmick and Victor Eijkhouta and Yoav Freund and Erika Fuentes and David Keye},
  Journal = {Int. J. High Perf. Comput. Appl},
  Title = {Application of Machine Learning to the Selection of Sparse Linear Solvers},
  Year = {2006}
}

@inproceedings{ganapathi2009case,
  title={A case for machine learning to optimize multicore performance},
  author={Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
  booktitle={Proceedings of the First {USENIX} conference on {Hot} topics in parallelism},
  year={2009},
  organization={USENIX Association}
}

@inproceedings{duplyakin2016active,
  title={Active Learning in Performance Analysis},
  author={Duplyakin, Dmitry and Brown, Jed and Ricci, Robert},
  booktitle={Cluster Computing (CLUSTER), 2016 {IEEE} International Conference on},
  pages={182--191},
  year={2016},
  organization={IEEE}
}

@article{NorrisBJ14,
  Author = {Boyana Norris and Sa{-}Lin Bernstein and Ramya Nair and Elizabeth R. Jessup},
  Biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/NorrisBNJ14},
  Journal = {CoRR},
  Title = {Lighthouse: {A} User-Centered Web Service for Linear Algebra Software},
  Url = {http://arxiv.org/abs/1408.1363},
  Volume = {abs/1408.1363},
  Year = {2014},
}

@book{kullback1997information,
  title={Information theory and statistics},
  author={Kullback, Solomon},
  year={1997},
  publisher={Courier Corporation}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@misc{dewancker2015bayesian,
  title={Bayesian optimization primer},
  author={Dewancker, Ian and McCourt, Michael and Clark, Scott},
  year={2015}
}

@book{mockus2012bayesian,
  title={Bayesian approach to global optimization: theory and applications},
  author={Mockus, Jonas},
  volume={37},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@techreport{arnold:ibm-tr00,
  Author = {Matthew Arnold and Peter F. Sweeney},
  Institution = {IBM Research Division},
  Keywords = {cct,sampling,profiling,java},
  Month = {July 7},
  Number = {RC 21789 (98099)},
  Title = {Approximating the Calling Context Tree via Sampling},
  Year = {2000}
}

@inproceedings{tallent:ics11,
  Author = {N. Tallent and J. Mellor-Crummey and M. Franco and R. Landrum and L. Adhianto},
  Booktitle = ICS11,
  Month = jun,
  Title = {{Scalable Fine-grained Call Path Tracing}},
  Year = 2011
}

@ARTICLE{decision-trees,
  author={S. R. {Safavian} and D. {Landgrebe}},
  journal={IEEE Transactions on Systems, Man, and Cybernetics},
  title={A survey of decision tree classifier methodology},
  year={1991},
  volume={21},
  number={3},
  pages={660-674},
}

@article{Cortes:1995,
  Author = {Corinna Cortes and Vladimir Vapnik},
  Issue_Date = {Sept. 1995},
  Journal = {Mach. Learn.},
  Month = {September},
  Number = {3},
  Numpages = {25},
  Pages = {273--297},
  Title = {Support-Vector Networks},
  Volume = {20},
  Year = {1995}
}

@article{Smola:2004,
  Author = {Smola, Alex J. and Sch\"{o}lkopf, Bernhard},
  Issue_Date = {August 2004},
  Journal = {Statistics and Computing},
  Month = {August},
  Number = {3},
  Numpages = {24},
  Pages = {199--222},
  Title = {A Tutorial on Support Vector Regression},
  Volume = {14},
  Year = {2004}
}

@article{KalchbrennerGB14,
  author    = {Nal Kalchbrenner and
               Edward Grefenstette and
               Phil Blunsom},
  title     = {A Convolutional Neural Network for Modelling Sentences},
  journal   = {CoRR},
  volume    = {abs/1404.2188},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.2188},
  archivePrefix = {arXiv},
  eprint    = {1404.2188},
}

@ARTICLE{Bronstein:spm2017,
  author={M. M. {Bronstein} and J. {Bruna} and Y. {LeCun} and A. {Szlam} and P. {Vandergheynst}},
  journal={IEEE Signal Processing Magazine},
  title={Geometric Deep Learning: Going beyond Euclidean data},
  year={2017},
  volume={34},
  number={4},
  pages={18-42},
}

@incollection{Cuturi2013,
  title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
  author = {Cuturi, Marco},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages = {2292--2300},
  year = {2013},
  publisher = {Curran Associates, Inc.},
}

@article{Vallisneri2008,
  title = {Use and abuse of the Fisher information matrix in the assessment of gravitational-wave parameter-estimation prospects},
  author = {Vallisneri, Michele},
  journal = {Phys. Rev. D},
  volume = {77},
  issue = {4},
  pages = {042001},
  numpages = {20},
  year = {2008},
  month = {Feb},
  publisher = {American Physical Society},
}

@article{dietterich2000experimental,
    Author = {Dietterich, Thomas G},
    Journal = {Machine learning},
    Number = {2},
    Pages = {139--157},
    Publisher = {Springer},
    Title = {An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization},
    Volume = {40},
    Year = {2000}}

@article{friedman2002stochastic,
    Author = {Friedman, Jerome H},
    Journal = {Computational Statistics \& Data Analysis},
    Number = {4},
    Pages = {367--378},
    Publisher = {Elsevier},
    Title = {Stochastic gradient boosting},
    Volume = {38},
    Year = {2002}}

@article{geurts2006extremely,
    Author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
    Journal = {Machine learning},
    Number = {1},
    Pages = {3--42},
    Publisher = {Springer},
    Title = {Extremely randomized trees},
    Volume = {63},
    Year = {2006}}

@article{natekin2013gradient,
    Author = {Natekin, Alexey and Knoll, Alois},
    Journal = {Frontiers in neurorobotics},
    Publisher = {Frontiers Media SA},
    Title = {Gradient boosting machines, a tutorial},
    Volume = {7},
    Year = {2013}}

@article{TaiSM15,
  author    = {Kai Sheng Tai and
               Richard Socher and
               Christopher D. Manning},
  title     = {Improved Semantic Representations From Tree-Structured Long Short-Term
               Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1503.00075},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.00075},
  eprint    = {1503.00075},
}

@article{KipfW16,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1609.02907},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.02907},
  eprint    = {1609.02907},
}

@InProceedings{Xiao_2015_CVPR,
author = {Xiao, Tianjun and Xu, Yichong and Yang, Kuiyuan and Zhang, Jiaxing and Peng, Yuxin and Zhang, Zheng},
title = {The Application of Two-Level Attention Models in Deep Convolutional Neural Network for Fine-Grained Image Classification},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@techreport{Hornung:0AUN2GzA,
    Author = {Hornung, R D and Keasler, J A},
    Institution = {Lawrence Livermore National Laboratory},
    Month = sep,
    Number = {LLNL-TR-661403},
    Title = {{The RAJA Portability Layer: Overview and Status}},
    Year = {2014}}

@inproceedings{RadfordMC15,
  author    = {Alec Radford and
               Luke Metz and
               Soumith Chintala},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Unsupervised Representation Learning with Deep Convolutional Generative
               Adversarial Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
}

@inproceedings{PeKePa03,
    Author = {Petrini, Fabrizio and Kerbyson, Darren J. and Pakin, Scott},
    Booktitle = {Proceedings of the 2003 ACM/IEEE conference on Supercomputing (SC'03)},
    Location = {Phoenix, AZ, USA},
    Title = {{The Case of the Missing Supercomputer Performance: Achieving Optimal Performance on the 8,192 Processors of ASCI Q}},
    Year = {2003}
}

@INPROCEEDINGS{kim:dragonfly,
    author={J. Kim and W. J. Dally and S. Scott and D. Abts},
    booktitle={2008 International Symposium on Computer Architecture},
    title={Technology-Driven, Highly-Scalable Dragonfly Topology},
    year={2008},
    publisher = {IEEE Computer Society},
}

@misc{nersc:cori_system,
  title = {Cori System},
  url = {https://docs.nersc.gov/systems/cori/},
  publisher = {NERSC},
}

@misc{alcf:theta,
  title = {Theta System},
  url = {https://www.alcf.anl.gov/alcf-resources/theta},
  publisher = {ALCF},
}

@INPROCEEDINGS{groves:perf_variability_on_dragonfly,
    author={T. {Groves} and Y. {Gu} and N. J. {Wright}},
    booktitle={2017 IEEE International Conference on Cluster Computing (CLUSTER)},
    title={Understanding Performance Variability on the Aries Dragonfly Network},
    year={2017},
    volume={},
    number={},
    pages={809-813},
}

@article{prettenhofer2014gradient,
  title={Gradient boosted regression trees in scikit-learn},
  author={Prettenhofer, Peter and Louppe, Gilles},
  year={2014}
}

@article{freund2003efficient,
  title={An efficient boosting algorithm for combining preferences},
  author={Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={4},
  number={Nov},
  pages={933--969},
  year={2003}
}

@article{srivastava2013improving,
  title={Improving neural networks with dropout},
  author={Srivastava, Nitish},
  journal={University of Toronto},
  volume={182},
  number={566},
  pages={7},
  year={2013}
}

@inproceedings{Chunduri:2017,
	Address = {New York, NY, USA},
	Author = {Chunduri, Sudheer and Harms, Kevin and Parker, Scott and Morozov, Vitali and Oshin, Samuel and Cherukuri, Naveen and Kumaran, Kalyan},
	Booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	Location = {Denver, Colorado},
	Publisher = {ACM},
	Series = {SC '17},
	Title = {Run-to-run Variability on Xeon Phi Based Cray XC Systems},
	Year = {2017},
}

@inproceedings{jha:cluster2018,
    Author = {S. {Jha} and J. {Brandt} and A. {Gentile} and Z. {Kalbarczyk} and R. {Iyer}},
    Booktitle = {2018 IEEE International Conference on Cluster Computing (CLUSTER)},
    Doi = {10.1109/CLUSTER.2018.00072},
    Month = {Sep.},
    Pages = {562-570},
    Title = {Characterizing Supercomputer Traffic Networks Through Link-Level Analysis},
    Year = {2018},
}

@inproceedings{skinner2005understanding,
	Author = {Skinner, David and Kramer, William},
	Booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
	Organization = {IEEE},
	Pages = {137--149},
	Title = {Understanding the causes of performance variability in HPC workloads},
	Year = {2005}
}

@article{tuncer:tpds2019,
  author={O. {Tuncer} and E. {Ates} and Y. {Zhang} and A. {Turk} and J. {Brandt} and V. J. {Leung} and M. {Egele} and A. K. {Coskun}},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  title={Online Diagnosis of Performance Variation in HPC Systems Using Machine Learning},
  year={2019},
  volume={30},
  pages={883-896}
}

@article{sandri2010analysis,
  title={Analysis and correction of bias in total decrease in node impurity measures for tree-based algorithms},
  author={Sandri, Marco and Zuccolotto, Paola},
  journal={Statistics and Computing},
  volume={20},
  number={4},
  pages={393--407},
  year={2010},
  publisher={Springer}
}

@misc{ariescounters,
    Howpublished = {\url{http://docs.cray.com/books/S-0045-20/S-0045-20.pdf}},
    Title = {Aries Hardware Counters (S-0045-20)},
    Year = {2017}
}

@article{milc00,
	Author = {Claude Bernard and Tom Burch and Thomas A. DeGrand and Carleton DeTar and Steven Gottlieb and Urs M. Heller and James E. Hetrick and Kostas Orginos and Bob Sugar and Doug Toussaint},
	Journal = {Physical Review D},
	Number = {61},
	Title = {{Scaling tests of the improved Kogut-Susskind quark action}},
	Year = 2000}

@INPROCEEDINGS{vite:ipdps18,
    author={S. {Ghosh} and M. {Halappanavar} and A. {Tumeo} and A. {Kalyanaraman} and H. {Lu} and D. {Chavarrià-Miranda} and A. {Khan} and A. {Gebremedhin}},
    booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
    title={Distributed Louvain Algorithm for Graph Community Detection},
    year={2018},
    pages={885-895},
    doi={10.1109/IPDPS.2018.00098},
    ISSN={1530-2075},
    month={May}
}

@INPROCEEDINGS{minivite:pmbs18,
    author={S. {Ghosh} and M. {Halappanavar} and A. {Tumeo} and A. {Kalyanaraman} and A. H. {Gebremedhin}},
    booktitle={2018 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)},
    title={MiniVite: A Graph Analytics Benchmarking Tool for Massively Parallel Systems},
    year={2018},
    pages={51-56},
    doi={10.1109/PMBS.2018.8641631},
    month={Nov},
}

@article{friedman2001greedy,
	Author = {Friedman, Jerome H.},
	Journal = {The Annals of Statistics},
	Number = {5},
	Pages = {pp. 1189-1232},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Greedy Function Approximation: A Gradient Boosting Machine},
	Volume = {29},
	Year = {2001}
}

@phdthesis{ugal,
	Author = {Arjun Singh},
	Note = {\url{http://cva.stanford.edu/publications/2005/thesis_arjuns.pdf}},
	School = {Dept. of Electrical Engineering, Stanford University},
	Title = {Load-Balanced Routing in Interconnection Networks},
	Year = {2005}
}

@inproceedings{wolski:sc97,
  author = {Wolski, Rich and Spring, Neil and Peterson, Chris},
  title = {Implementing a Performance Forecasting System for Metacomputing: The Network Weather Service},
  year = {1997},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/509593.509600},
  doi = {10.1145/509593.509600},
  booktitle = {Proceedings of the 1997 ACM/IEEE Conference on Supercomputing},
  location = {San Jose, CA},
  series = {SC ’97}
}

@inproceedings{hoefler:sc2010,
  author = {Hoefler, Torsten and Schneider, Timo and Lumsdaine, Andrew},
  title = {Characterizing the Influence of System Noise on Large-Scale Applications by Simulation},
  year = {2010},
  publisher = {IEEE Computer Society},
  address = {USA},
  url = {https://doi.org/10.1109/SC.2010.12},
  booktitle = {Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis},
  series = {SC ’10}
}

@article{agelastos:parco2016,
  author = "Anthony Agelastos and Benjamin Allan and Jim Brandt and Ann Gentile and Sophia Lefantzi and Steve Monk and Jeff Ogden and Mahesh Rajan and Joel Stevenson",
  title = "Continuous whole-system monitoring toward rapid understanding of production HPC applications and systems",
  journal = "Parallel Computing",
  volume = "58",
  pages = "90 - 106",
  year = "2016",
  issn = "0167-8191",
  doi = "https://doi.org/10.1016/j.parco.2016.05.009",
  url = "http://www.sciencedirect.com/science/article/pii/S0167819116300394",
}

@incollection{hoppe:2020,
  author = {Hoppe, Dennis and Zhong, Li and Andersson, Stefan and Moise, Diana},
  title = {On the Detection and Interpretation of Performance Variations of HPC Applications},
  year = {2020},
  month = {03},
  pages = {41-56},
  booktitle = {Sustained Simulation Performance 2018 and 2019},
  publisher = {Springer},
  editor = {M. Resch and Y. Kovalenko and W. Bez and E. Focht and H. Kobayashi}
}

@INPROCEEDINGS{chunduri:pmbs2019,
  author={S. {Chunduri} and E. {Jennings} and K. {Harms} and C. {Knight} and S. {Parker}},
  booktitle={2019 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)}, 
  title={A Generalized Statistics-Based Model for Predicting Network-Induced Variability}, 
  year={2019},
  pages={59-72},
}

@inproceedings{MONET:NSDI:2020,
 author = {S. Jha and  A. Patke and B. Lim and  J. Brandt and A. Gentile and G. Bauer and M. Showerman and L. Kaplan and Z. Kalbarczyk and W. T. Kramer and R. Iyer},
 booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
 month = {Feb},
 title = {Measuring Congestion in High-Performance Datacenter Interconnects},
 year = {2020}
}

@INPROCEEDINGS{wyatt:ipdps2020,
  author={M. R. {Wyatt} and S. {Herbein} and K. {Shoga} and T. {Gamblin} and M. {Taufer}},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  title={CanarIO: Sounding the Alarm on IO-Related Performance Degradation},
  year={2020},
  volume={},
  number={},
  pages={73-83},
}

@INPROCEEDINGS{di:dsn2019,
  author={S. {Di} and H. {Guo} and E. {Pershey} and M. {Snir} and F. {Cappello}},
  booktitle={2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
  title={Characterizing and Understanding HPC Job Failures Over The 2K-Day Life of IBM BlueGene/Q System},
  year={2019},
  volume={},
  number={},
  pages={473-484},
}

@inproceedings{nowak1999,
    Author = {Nowak, P. F. and Nemanic, M. K.},
    Booktitle = {Proceedings of the International Conference on Mathematics and Computation, Reactor Physics and Environmental Analysis in Nuclear Applications},
    Month = sep,
    Title = {Radiation Transport Calculations on Unstructured Grids Using a Spatially Decomposed and Threaded Algorithm},
    Year = 1999
}

@inproceedings{mou:aaai2016,
  author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
  title = {Convolutional Neural Networks over Tree Structures for Programming Language Processing},
  year = {2016},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  pages = {1287–1293},
  numpages = {7},
  location = {Phoenix, Arizona},
  series = {AAAI’16}
}

@INPROCEEDINGS{subramoni:cluster2013,
  author={H. {Subramoni} and D. {Bureddy} and K. {Kandalla} and K. {Schulz} and B. {Barth} and J. {Perkins} and M. {Arnold} and D. K. {Panda}},
  booktitle={2013 IEEE International Conference on Cluster Computing (CLUSTER)}, 
  title={Design of network topology aware scheduling services for large InfiniBand clusters}, 
  year={2013},
}

@INPROCEEDINGS{lu:globecom2019,
  author={J. {Lu} and P. {Li} and K. {Wang} and H. {Feng} and E. {Guo} and X. {Wang} and S. {Guo}},
  booktitle={2019 IEEE Global Communications Conference (GLOBECOM)}, 
  title={Topology-Aware Job Scheduling for Machine Learning Cluster}, 
  year={2019},
}

@INPROCEEDINGS{ldms,
  author={A. {Agelastos} and B. {Allan} and J. {Brandt} and P. {Cassella} and J. {Enos} and J. {Fullop} and A. {Gentile} and S. {Monk} and N. {Naksinehaboon} and J. {Ogden} and M. {Rajan} and M. {Showerman} and J. {Stevenson} and N. {Taerat} and T. {Tucker}},
  booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={The Lightweight Distributed Metric Service: A Scalable Infrastructure for Continuous Monitoring of Large Scale Computing Systems and Applications}, 
  year={2014},
  volume={},
  number={},
  pages={154-165},
}

@article{graham1982gprof,
    Author = {Graham, Susan L and Kessler, Peter B and Mckusick, Marshall K},
    Date-Modified = {2018-12-08 06:15:33 +0000},
    Journal = {SIGPLAN Not.},
    Number = {6},
    Pages = {120--126},
    Publisher = {ACM},
    Title = {Gprof: A call graph execution profiler},
    Volume = {17},
    Year = {1982}}
}

@article{mellorcrummey+:jsc02,
    Author = {J. Mellor-Crummey and R. Fowler and G. Marin},
    Date-Added = {2012-01-16 03:42:52 -0800},
    Date-Modified = {2012-01-16 03:42:52 -0800},
    Journal = {The Journal of Supercomputing},
    Pages = {81-101},
    Title = {{HPCView: A tool for top-down analysis of node performance}},
    Volume = 23,
    Year = 2002
}

@misc{gregg:flame2015,
    Author = {Gregg, Brendan},
    Date-Added = {2019-04-11 00:12:41 -0700},
    Date-Modified = {2019-04-11 00:42:39 -0700},
    Howpublished = {Online},
    Note = {http://www.brendangregg.com/Slides/FreeBSD2014\_FlameGraphs.pdf},
    Title = {Flame graphs},
    Year = {2015}
}

@inproceedings{huck+:perfdmf2005,
    Author = {Kevin Huck and Allen D. Malony and R Bell and L Li and A Morris},
    Booktitle = {International Conference on Parallel Processing (ICPP'05)},
    Date-Added = {2019-04-11 00:06:10 -0700},
    Date-Modified = {2019-04-11 00:07:27 -0700},
    Title = {PerfDMF: Design and implementation of a parallel performance data management framework},
    Year = {2005}
}

@inproceedings{mckenney:differential1995,
    Author = {P. E. {McKenney}},
    Booktitle = {MASCOTS '95. Proceedings of the Third International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems},
    Date-Modified = {2019-04-11 00:08:35 -0700},
    Doi = {10.1109/MASCOT.1995.378681},
    Month = {Jan},
    Pages = {237-241},
    Title = {Differential profiling},
    Year = {1995},
    Bdsk-Url-1 = {https://doi.org/10.1109/MASCOT.1995.378681}
}

@inproceedings{schulz+:differential07,
    Address = {Berlin, Heidelberg},
    Author = {Schulz, Martin and de Supinski, Bronis R.},
    Booktitle = {Euro-Par 2007 Parallel Processing},
    Date-Modified = {2019-04-11 00:09:03 -0700},
    Editor = {Kermarrec, Anne-Marie and Boug{\'e}, Luc and Priol, Thierry},
    Isbn = {978-3-540-74466-5},
    Pages = {97--106},
    Publisher = {Springer Berlin Heidelberg},
    Title = {Practical Differential Profiling},
    Year = {2007}
}

@inproceedings{tallent+:sc09,
    Author = {Nathan R. Tallent and John M. Mellor-Crummey and Laksono Adhianto and Michael W. Fagan and Mark Krentel},
    Booktitle = super09,
    Date-Added = {2012-01-16 03:17:15 -0800},
    Date-Modified = {2012-01-16 03:17:15 -0800},
    Month = nov,
    Title = {{Diagnosing performance bottlenecks in emerging petascale applications}},
    Year = 2011
}

@inproceedings{tallent+:sc10,
    Author = {Nathan R. Tallent and Laksono Adhianto and John M. Mellor-Crummey},
    Booktitle = super10,
    Date-Added = {2012-01-16 03:17:15 -0800},
    Date-Modified = {2012-01-16 03:17:15 -0800},
    Month = nov,
    Title = {Scalable Identification of Load Imbalance in Parallel Executions Using Call Path Profiles},
    Year = 2010
}

@article{shende:tau2006,
  title={The TAU parallel performance system},
  author={Shende, Sameer S and Malony, Allen D},
  journal={The International Journal of High Performance Computing Applications},
  volume={20},
  number={2},
  pages={287--311},
  year={2006},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{bell:paraprof2003,
  title={Paraprof: A portable, extensible, and scalable tool for parallel performance profile analysis},
  author={Bell, Robert and Malony, Allen D and Shende, Sameer},
  booktitle={European Conference on Parallel Processing},
  pages={17--26},
  year={2003},
  organization={Springer}
}

@inproceedings{huck:perfexplorer2005,
  title={Perfexplorer: A performance data mining framework for large-scale parallel computing},
  author={Huck, Kevin A and Malony, Allen D},
  booktitle={SC'05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing},
  pages={41--41},
  year={2005},
  organization={IEEE}
}

@misc{openspeedshop,
  author = {The Open|SpeedShop Team.},
  note={Accessed November 2020},
  title = { Open|SpeedShop for Linux},
  howpublished = {https://openspeedshop.org/}
}

@misc{team2013r,
  title={R: A language and environment for statistical computing},
  author={Team, R Core and others},
  year={2013},
  publisher={Vienna, Austria}
}

@book{mckinney:pandas,
    Author = {Wes McKinney},
    Date-Added = {2019-04-10 23:58:27 -0700},
    Date-Modified = {2019-04-10 23:58:56 -0700},
    Isbn = {1491957662},
    Publisher = {O'Reilly Media},
    Title = {{Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython}},
    Year = {2017},
}

@InProceedings{mckinney:pandas2,
  author    = { Wes McKinney },
  title     = { Data Structures for Statistical Computing in Python },
  booktitle = { Proceedings of the 9th Python in Science Conference },
  pages     = { 51 - 56 },
  year      = { 2010 },
  editor    = { St\'efan van der Walt and Jarrod Millman }
}

@INPROCEEDINGS{song:icpp2004,
  author={F. {Song} and F. {Wolf} and N. {Bhatia} and J. {Dongarra} and S. {Moore}},
  booktitle={International Conference on Parallel Processing, 2004. ICPP 2004.}, 
  title={An algebra for cross-experiment performance analysis}, 
  year={2004},
  volume={},
  number={},
  pages={63-72 vol.1},
  doi={10.1109/ICPP.2004.1327905}
}

@inproceedings{acun:sc14,
    Author = {Acun, Bilge and Gupta, Abhishek and Jain, Nikhil and Langer, Akhil and Menon, Harshitha and Mikida, Eric and Ni, Xiang and Robson, Michael and Sun, Yanhua and Totoni, Ehsan and Wesolowski, Lukasz and Kale, Laxmikant},
    Series = {SC},
    Title = {{Parallel Programming with Migratable Objects: Charm++ in Practice}},
    Year = {2014}
}

@inproceedings{Kaiser.PGAS.14,
 author = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
 title = {HPX: A Task Based Programming Model in a Global Address Space},
 booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
 series = {PGAS '14},
 year = {2014},
 isbn = {978-1-4503-3247-7},
 location = {Eugene, OR, USA},
 pages = {6:1--6:11},
 articleno = {6},
 numpages = {11},
 @url = {http://doi.acm.org/10.1145/2676870.2676883},
 @doi = {10.1145/2676870.2676883},
 acmid = {2676883},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@MISC{OpenMP4,
  title = {{OpenMP Application Program Interface. Version 4.0. July 2013}},
  key = {OpenMP4},
  year = {2013}
}

@ARTICLE{Duran.2011.PPL,
  author = {Duran, Alejandro and Ayguad{\'e}, Eduard and Badia, Rosa M and Labarta,
  Jes{\'u}s and Martinell, Luis and Martorell, Xavier and Planas, Judit},
  title = {{OmpSs}: A Proposal for Programming Heterogeneous Multi-core Architectures},
  journal = {Parallel Processing Letters},
  month = jun,
  year = {2011},
  volume = {21},
  pages = {173--193},
  number = {2},
  publisher = {World Scientific}
}

@INPROCEEDINGS{Bauer.2012.SC,
  author = {Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken,
  Alex},
  title = {Legion: Expressing Locality and Independence with Logical Regions},
  booktitle = {Proceedings of the 2012 ACM/IEEE International Conference on High
  Performance Computing, Networking, Storage and Analysis},
  year = {2012},
  series = {SC '12},
  pages = {66:1--66:11},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  acmid = {2389086},
  articleno = {66},
  isbn = {978-1-4673-0804-5},
  location = {Salt Lake City, Utah},
  numpages = {11},
}

@misc{modin,
  author = {Modin Developers},
  note={Accessed November 2020},
  title = { Modin: Speed up your Pandas workflows by changing a single line of code},
  howpublished = {https://modin.readthedocs.io/en/latest/}
}

@inproceedings{CharmppOOPSLA93,
    Author = {{Kal\'{e}}, L.V. and Krishnan, S.},
    Booktitle = {{Proceedings of OOPSLA'93}},
    Editor = {Paepcke, A.},
    Fulleditor = {Paepcke, Andreas},
    Month = {September},
    Pages = {91--108},
    Publisher = {{ACM Press}},
    Title = {{CHARM++: A Portable Concurrent Object Oriented System Based on C++}},
    Year = {1993}
}

@misc{charm4py,
  title = {Charm4py},
  howpublished = {https://charm4py.readthedocs.io}
}

@INPROCEEDINGS{charmpy,
  author={J. J. {Galvez} and K. {Senthil} and L. {Kale}},
  booktitle={2018 IEEE International Conference on Cluster Computing (CLUSTER)}, 
  title={CharmPy: A Python Parallel Programming Model}, 
  year={2018},
  volume={},
  number={},
  pages={423-433},
  doi={10.1109/CLUSTER.2018.00059}}

@Article{Jones2008,
  author={Jones, Kate E.
    and Patel, Nikkita G.
      and Levy, Marc A.
      and Storeygard, Adam
      and Balk, Deborah
      and Gittleman, John L.
      and Daszak, Peter},
  title={Global trends in emerging infectious diseases},
  journal={Nature},
  year={2008},
  month={Feb},
  day={01},
  volume={451},
  number={7181},
  pages={990-993},
  abstract={Emerging infectious diseases are a major threat to health: AIDS, SARS, drug-resistant bacteria and Ebola virus are among the more recent examples. By identifying emerging disease 'hotspots', the thinking goes, it should be possible to spot health risks at an early stage and prepare containment strategies. An analysis of over 300 examples of disease emerging between 1940 and 2004 suggests that these hotspots can be accurately mapped based on socio-economic, environmental and ecological factors. The data show that the surveillance effort, and much current research spending, is concentrated in developed economies, yet the risk maps point to developing countries as the more likely source of new diseases.},
  issn={1476-4687},
  doi={10.1038/nature06536},
  url={https://doi.org/10.1038/nature06536}
}

@article{hethcote2000mathematics,
  title={The mathematics of infectious diseases},
  author={Hethcote, Herbert W},
  journal={SIAM review},
  volume={42},
  number={4},
  pages={599--653},
  year={2000},
  publisher={SIAM}
}

@article{cuDNN,
  author    = {Sharan Chetlur and
               Cliff Woolley and
               Philippe Vandermersch and
               Jonathan Cohen and
               John Tran and
               Bryan Catanzaro and
               Evan Shelhamer},
  title     = {cuDNN: Efficient Primitives for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1410.0759},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.0759},
  archivePrefix = {arXiv},
  eprint    = {1410.0759},
  timestamp = {Mon, 13 Aug 2018 16:48:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChetlurWVCTCS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{jeh2019miopen,
    title={MIOpen: An Open Source Library For Deep Learning Primitives},
    author={Jehandad Khan and Paul Fultz and Artem Tamazov and Daniel Lowell and Chao Liu and Michael Melesse and Murali Nandhimandalam and Kamil Nasyrov and Ilya Perminov and Tejash Shah and Vasilii Filippov and Jing Zhang and Jing Zhou and Bragadeesh Natarajan and Mayank Daga},
    year={2019},
    eprint={1910.00078},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{gamma-iccad2019,
	author = {Kao, Sheng-Chun and Krishna, Tushar},
	title = {GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm},
	year = {2020},
	isbn = {9781450380263},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3400302.3415639},
	doi = {10.1145/3400302.3415639},
	booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
	articleno = {44},
	numpages = {9},
	keywords = {ML accelerator, reconfigurable device, genetic algorithm},
	location = {Virtual Event, USA},
	series = {ICCAD '20}
}

@ARTICLE{maestro-micro2020,  
	author={H. {Kwon} and P. {Chatarasi} and V. {Sarkar} and T. {Krishna} and M. {Pellauer} and A. {Parashar}},  
	journal={IEEE Micro},   
	title={MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings},   
	year={2020},  
	volume={40},  
	number={3},  
	pages={20-29},  
	doi={10.1109/MM.2020.2985963}
}

@inproceedings{lift-cgo2017,
	author = {Steuwer, Michel and Remmelg, Toomas and Dubach, Christophe},
	title = {Lift: A Functional Data-Parallel IR for High-Performance GPU Code Generation},
	year = {2017},
	isbn = {9781509049318},
	publisher = {IEEE Press},
	booktitle = {Proceedings of the 2017 International Symposium on Code Generation and Optimization},
	pages = {74–85},
	numpages = {12},
	location = {Austin, USA},
	series = {CGO '17}
}

@inproceedings{cnn-fpga-isfpg2017,
	author = {Ma, Yufei and Cao, Yu and Vrudhula, Sarma and Seo, Jae-sun},
	title = {Optimizing Loop Operation and Dataflow in FPGA Acceleration of Deep Convolutional Neural Networks},
	year = {2017},
	isbn = {9781450343541},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3020078.3021736},
	doi = {10.1145/3020078.3021736},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {45–54},
	numpages = {10},
	keywords = {hardware acceleration, convolutional neural networks, FPGA},
	location = {Monterey, California, USA},
	series = {FPGA '17}
}

@INPROCEEDINGS{neurocube-isca2016,  
	author={D. {Kim} and J. {Kung} and S. {Chai} and S. {Yalamanchili} and S. {Mukhopadhyay}},  
	booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},   
	title={Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory},   
	year={2016},  
	volume={},  
	number={},  
	pages={380-392},  
	doi={10.1109/ISCA.2016.41}
}

@INPROCEEDINGS{eyeriss-isca2016,  
	author={Y. {Chen} and J. {Emer} and V. {Sze}},  
	booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},   
	title={Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},   
	year={2016},  
	volume={},  
	number={},  
	pages={367-379},  
	doi={10.1109/ISCA.2016.40}
}

@ARTICLE{eyeriss-ssc2017,  
	author={Y. {Chen} and T. {Krishna} and J. S. {Emer} and V. {Sze}},  
	journal={IEEE Journal of Solid-State Circuits},   
	title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},   
	year={2017},  
	volume={52},  
	number={1},  
	pages={127-138},  
	doi={10.1109/JSSC.2016.2616357}
}

@INPROCEEDINGS{hypar-hpca2019,  
	author={L. {Song} and J. {Mao} and Y. {Zhuo} and X. {Qian} and H. {Li} and Y. {Chen}},  
	booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   
	title={HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array},   
	year={2019},  
	volume={},  
	number={},  
	pages={56-68},  
	doi={10.1109/HPCA.2019.00027}
}

@book{CharmAppsBook:2013,
  editor = {Kale, Laxmikant V. and Bhatele, Abhinav},
  title = "{Parallel Science and Engineering Applications: The Charm++ Approach}",
  year = {2013},
  month = nov,
  publisher = {Taylor \& Francis Group, CRC Press},
  isbn = {9781466504127}
}

@inproceedings{gamblin:sc15,
  author = {T. Gamblin and M. LeGendre and M. R. Collette and G. L. Lee and A. Moody and B. R. de Supinski and S. Futral},
  booktitle = {SC15: International Conference for High-Performance Computing, Networking, Storage and Analysis},
  title = {The Spack package manager: bringing order to HPC software chaos},
  year = {2015},
  issn = {2167-4337},
  doi = {10.1145/2807591.2807623},
  url = {https://doi.ieeecomputersociety.org/10.1145/2807591.2807623},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  month = {nov}
}

@inproceedings{seal:simutools2010,
  author = {Aaby, Brandon G. and Perumalla, Kalyan S. and Seal, Sudip K.},
  title = {Efficient Simulation of Agent-Based Models on Multi-GPU and Multi-Core Clusters},
  year = {2010},
  publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
  address = {Brussels, BEL},
  url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8822},
  booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
  location = {Torremolinos, Malaga, Spain},
  series = {SIMUTools '10}
}

@article{grefenstette2013fred,
  title={FRED (A Framework for Reconstructing Epidemic Dynamics): an open-source software system for modeling infectious diseases and control strategies using census-based populations},
  author={Grefenstette, John J and Brown, Shawn T and Rosenfeld, Roni and DePasse, Jay and Stone, Nathan TB and Cooley, Phillip C and Wheaton, William D and Fyshe, Alona and Galloway, David D and Sriram, Anuroop and others},
  journal={BMC public health},
  volume={13},
  number={1},
  pages={1--14},
  year={2013},
  publisher={BioMed Central}
}

@article{parker2011distributed,
  title={A distributed platform for global-scale agent-based models of disease transmission},
  author={Parker, Jon and Epstein, Joshua M},
  journal={ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  volume={22},
  number={1},
  pages={1--25},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{perumalla2012discrete,
  title={Discrete event modeling and massively parallel execution of epidemic outbreak phenomena},
  author={Perumalla, Kalyan S and Seal, Sudip K},
  journal={Simulation},
  volume={88},
  number={7},
  pages={768--783},
  year={2012},
  publisher={Sage Publications Sage UK: London, England}
}

@article{eubank2004modelling,
  title={Modelling disease outbreaks in realistic urban social networks},
  author={Eubank, Stephen and Guclu, Hasan and Kumar, VS Anil and Marathe, Madhav V and Srinivasan, Aravind and Toroczkai, Zoltan and Wang, Nan},
  journal={Nature},
  volume={429},
  number={6988},
  pages={180--184},
  year={2004},
  publisher={Nature Publishing Group}
}

@article{longini2005containing,
  title={Containing pandemic influenza at the source},
  author={Longini, Ira M and Nizam, Azhar and Xu, Shufu and Ungchusak, Kumnuan and Hanshaoworakul, Wanna and Cummings, Derek AT and Halloran, M Elizabeth},
  journal={Science},
  volume={309},
  number={5737},
  pages={1083--1087},
  year={2005},
  publisher={American Association for the Advancement of Science}
}

@article{ferguson2003planning,
  title={Planning for smallpox outbreaks},
  author={Ferguson, Neil M and Keeling, Matt J and Edmunds, W John and Gani, Raymond and Grenfell, Bryan T and Anderson, Roy M and Leach, Steve},
  journal={Nature},
  volume={425},
  number={6959},
  pages={681--685},
  year={2003},
  publisher={Nature Publishing Group}
}

@manual{google2020protobuf,
  title={Protocol Buffers},
  author={Google},
  howpublished="ver. 3.14.0",
  year={2020},
  url={https://github.com/protocolbuffers/protobuf/releases/tag/v3.14.0}
}

@manual{ppl2020charm,
  title={Charm++},
  author={Parallel Programing Laboratory},
  howpublished="ver 6.10.2",
  year={2020},
  url={https://github.com/UIUC-PPL/charm/releases/tag/v6.10.2}
}

@inproceedings{barrett2008episimdemics,
  title={EpiSimdemics: an efficient algorithm for simulating the spread of infectious disease over large realistic social networks},
  author={Barrett, Christopher L and Bisset, Keith R and Eubank, Stephen G and Feng, Xizhou and Marathe, Madhav V},
  booktitle={SC'08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing},
  pages={1--12},
  year={2008},
  organization={IEEE}
}

@inproceedings{yeom2014overcoming,
  title={Overcoming the scalability challenges of epidemic simulations on blue waters},
  author={Yeom, Jae-Seung and Bhatele, Abhinav and Bisset, Keith and Bohm, Eric and Gupta, Abhishek and Kale, Laxmikant V and Marathe, Madhav and Nikolopoulos, Dimitrios S and Schulz, Martin and Wesolowski, Lukasz},
  booktitle={2014 IEEE 28th International Parallel and Distributed Processing Symposium},
  pages={755--764},
  year={2014},
  organization={IEEE}
}

@inproceedings{bhatele2017massively,
  title={Massively parallel simulations of spread of infectious diseases over realistic social networks},
  author={Bhatele, Abhinav and Yeom, Jae-Seung and Jain, Nikhil and Kuhlman, Chris J and Livnat, Yarden and Bisset, Keith R and Kale, Laxmikant V and Marathe, Madhav V},
  booktitle={2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  pages={689--694},
  year={2017},
  organization={IEEE}
}

@article{bennun2019demystifying,
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  title = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
  year = {2019},
  issue_date = {September 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {52},
  number = {4},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/3320060},
  doi = {10.1145/3320060},
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  journal = {ACM Comput. Surv.},
  month = aug,
  articleno = {65},
  numpages = {43},
  keywords = {Deep learning, parallel algorithms, distributed computing}
}

@article{pouyanfar2018dlsurvey,
  author = {Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, S. S.},
  title = {A Survey on Deep Learning: Algorithms, Techniques, and Applications},
  year = {2018},
  issue_date = {January 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {51},
  number = {5},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/3234150},
  doi = {10.1145/3234150},
  abstract = {The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.},
  journal = {ACM Comput. Surv.},
  month = sep,
  articleno = {92},
  numpages = {36},
  keywords = {neural networks, machine learning, distributed processing, big data, Deep learning, survey}
}

@ARTICLE{lu2018dlbigdata,  
  author={X. {Lu} and H. {Shi} and R. {Biswas} and M. H. {Javed} and D. K. {Panda}},  
  journal={IEEE Transactions on Multi-Scale Computing Systems},   
  title={DLoBD: A Comprehensive Study of Deep Learning over Big Data Stacks on HPC Clusters},   
  year={2018},  
  volume={4},  
  number={4},  
  pages={635-648},  
  doi={10.1109/TMSCS.2018.2845886}
}

@ARTICLE{awan2020dlcommunication,  
  author={A. A. {Awan} and A. {Jain} and C. {Chu} and H. {Subramoni} and D. K. {Panda}},  
  journal={IEEE Micro},   
  title={Communication Profiling and Characterization of Deep-Learning Workloads on Clusters With High-Performance Interconnects},   
  year={2020},  
  volume={40},  
  number={1},  
  pages={35-43},  
  doi={10.1109/MM.2019.2949986}
}

@misc{tang2020communicationefficient,
  title={Communication-Efficient Distributed Deep Learning: A Comprehensive Survey}, 
  author={Zhenheng Tang and Shaohuai Shi and Xiaowen Chu and Wei Wang and Bo Li},
  year={2020},
  eprint={2003.06307},
  archivePrefix={arXiv},
  primaryClass={cs.DC}
}

@inproceedings{dryden2019channelparrallelism,
  author = {Dryden, Nikoli and Maruyama, Naoya and Moon, Tim and Benson, Tom and Snir, Marc and Van Essen, Brian},
  title = {Channel and Filter Parallelism for Large-Scale CNN Training},
  year = {2019},
  isbn = {9781450362290},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3295500.3356207},
  doi = {10.1145/3295500.3356207},
  abstract = {Accelerating large-scale CNN training is needed to keep training times reasonable as datasets grow larger and models become more complex. Existing frameworks primarily scale using data-parallelism, but this is limited by the mini-batch size, which cannot grow arbitrarily. We introduce three algorithms that partition channel or filter data to exploit parallelism beyond the sample dimension. Further, they partition the parameters of convolutional layers, replacing global all reduces with segmented allreduces---smaller, concurrent allreduces among disjoint processor sets. These algorithms enable strong scaling, reduced communication overhead, and reduced memory pressure, enabling training of very wide CNNs.We demonstrate improved strong and weak scaling, including up to 4.1x reductions in training time for residual networks and 4x reductions in allreduce overhead. We also show that wider models provide improved accuracy on ImageNet. We study the current limitations of our algorithms and provide a direction for future optimizations of large-scale deep learning frameworks.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {10},
  numpages = {20},
  keywords = {algorithms, scaling, deep learning, CNN, convolution},
  location = {Denver, Colorado},
  series = {SC '19}
}

@inproceedings{essen2015lbann,
  author    = {Brian Van Essen and
               Hyojin Kim and
               Roger A. Pearce and
               Kofi Boakye and
               Barry Chen},
  title     = {{LBANN:} livermore big artificial neural network {HPC} toolkit},
  booktitle = {Proceedings of the Workshop on Machine Learning in High-Performance
               Computing Environments, {MLHPC} 2015, Austin, Texas, USA, November
               15, 2015},
  pages     = {5:1--5:6},
  publisher = {{ACM}},
  year      = {2015},
  url       = {https://doi.org/10.1145/2834892.2834897},
  doi       = {10.1145/2834892.2834897},
  timestamp = {Tue, 06 Nov 2018 16:59:29 +0100},
  biburl    = {https://dblp.org/rec/conf/sc/EssenKPBC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{sze2017efficientdnn,  
  author={V. {Sze} and Y. {Chen} and T. {Yang} and J. S. {Emer}},  
  journal={Proceedings of the IEEE},   
  title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey},   
  year={2017},  
  volume={105}, 
  number={12},  
  pages={2295-2329},  
  doi={10.1109/JPROC.2017.2761740}
}
@inproceedings{parunak1998agent,
  title={Agent-based modeling vs. equation-based modeling: A case study and users’ guide},
  author={Parunak, H Van Dyke and Savit, Robert and Riolo, Rick L},
  booktitle={International Workshop on Multi-Agent Systems and Agent-Based Simulation},
  pages={10--25},
  year={1998},
  organization={Springer}
}

@article{giordano2020modelling,
  title={Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy},
  author={Giordano, Giulia and Blanchini, Franco and Bruno, Raffaele and Colaneri, Patrizio and Di Filippo, Alessandro and Di Matteo, Angela and Colaneri, Marta},
  journal={Nature medicine},
  volume={26},
  number={6},
  pages={855--860},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{tiwari2020modelling,
  title={Modelling and analysis of COVID-19 epidemic in India},
  author={Tiwari, Alok},
  journal={Journal of Safety Science and Resilience},
  volume={1},
  number={2},
  pages={135--140},
  year={2020},
  publisher={Elsevier}
}

@article{silva2020covid,
  title={COVID-ABS: An agent-based model of COVID-19 epidemic to simulate health and economic effects of social distancing interventions},
  author={Silva, Petr{\^o}nio CL and Batista, Paulo VC and Lima, H{\'e}lder S and Alves, Marcos A and Guimar{\~a}es, Frederico G and Silva, Rodrigo CP},
  journal={Chaos, Solitons \& Fractals},
  volume={139},
  pages={110088},
  year={2020},
  publisher={Elsevier}
}

@article{cuevas2020agent,
  title={An agent-based model to evaluate the COVID-19 transmission risks in facilities},
  author={Cuevas, Erik},
  journal={Computers in biology and medicine},
  volume={121},
  pages={103827},
  year={2020},
  publisher={Elsevier}
}

@article{chinazzi2020effect,
  title={The effect of travel restrictions on the spread of the 2019 novel coronavirus (COVID-19) outbreak},
  author={Chinazzi, Matteo and Davis, Jessica T and Ajelli, Marco and Gioannini, Corrado and Litvinova, Maria and Merler, Stefano and y Piontti, Ana Pastore and Mu, Kunpeng and Rossi, Luca and Sun, Kaiyuan and others},
  journal={Science},
  volume={368},
  number={6489},
  pages={395--400},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@article{prem2020effect,
  title={The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study},
  author={Prem, Kiesha and Liu, Yang and Russell, Timothy W and Kucharski, Adam J and Eggo, Rosalind M and Davies, Nicholas and Flasche, Stefan and Clifford, Samuel and Pearson, Carl AB and Munday, James D and others},
  journal={The Lancet Public Health},
  volume={5},
  number={5},
  pages={e261--e270},
  year={2020},
  publisher={Elsevier}
}

@article{anastassopoulou2020data,
  title={Data-based analysis, modelling and forecasting of the COVID-19 outbreak},
  author={Anastassopoulou, Cleo and Russo, Lucia and Tsakris, Athanasios and Siettos, Constantinos},
  journal={PloS one},
  volume={15},
  number={3},
  pages={e0230405},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}


@Article{Naghshnejad2020,
  author={Naghshnejad, Mina
  and Singhal, Mukesh},
  title={A hybrid scheduling platform: a runtime prediction reliability aware scheduling platform to improve HPC scheduling performance},
  journal={The Journal of Supercomputing},
  year={2020},
  month={Jan},
  day={01},
  volume={76},
  number={1},
  pages={122-149},
  abstract={The performance of scheduling algorithms for HPC jobs highly depends on the accuracy of job runtime values. Prior research has established that neither user-provided runtimes nor system-generated runtime predictions are accurate. We propose a new scheduling platform that performs well in spite of runtime uncertainties. The key observation that we use for building our platform is the fact that two important classes of scheduling strategies (backfilling and plan based) differ in terms of sensitivity to runtime accuracy. We first confirm this observation by performing trace-based simulations to characterize the sensitivity of different scheduling strategies to job runtime accuracy. We then apply gradient boosting tree regression as a meta-learning approach to estimate the reliability of the system-generated job runtimes. The estimated prediction reliability of job runtimes is then used to choose a specific class of scheduling algorithm. Our hybrid scheduling platform uses a plan-based scheduling strategy for jobs with high expected runtime accuracy and backfills the remaining jobs on top of the planned jobs. While resource sharing is used to minimize fragmentation of resources, a specific ratio of CPU cores is reserved for backfilling of less predictable jobs to avoid starvation of these jobs. This ratio is adapted dynamically based on the resource requirement ratio of predictable jobs among recently submitted jobs. We perform extensive trace-driven simulations on real-world production traces to show that our hybrid scheduling platform outperforms both pure backfilling and pure plan-based scheduling algorithms.},
  issn={1573-0484},
  doi={10.1007/s11227-019-03004-3},
  url={https://doi.org/10.1007/s11227-019-03004-3}
}

@inproceedings{ml-scheduling-sc17,
  author = {Carastan-Santos, Danilo and de Camargo, Raphael Y.},
  title = {Obtaining Dynamic Scheduling Policies with Simulation and Machine Learning},
  year = {2017},
  isbn = {9781450351140},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3126908.3126955},
  doi = {10.1145/3126908.3126955},
  abstract = {Dynamic scheduling of tasks in large-scale HPC platforms is normally accomplished using ad-hoc heuristics, based on task characteristics, combined with some backfilling strategy. Defining heuristics that work efficiently in different scenarios is a difficult task, specially when considering the large variety of task types and platform architectures. In this work, we present a methodology based on simulation and machine learning to obtain dynamic scheduling policies. Using simulations and a workload generation model, we can determine the characteristics of tasks that lead to a reduction in the mean slowdown of tasks in an execution queue. Modeling these characteristics using a nonlinear function and applying this function to select the next task to execute in a queue improved the mean task slowdown in synthetic workloads. When applied to real workload traces from highly different machines, these functions still resulted in performance improvements, attesting the generalization capability of the obtained heuristics.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {32},
  numpages = {13},
  keywords = {high performance computing, scheduling, machine learning, simulation},
  location = {Denver, Colorado},
  series = {SC '17}
}

@INPROCEEDINGS{ml-backfilling-sc15,  
  author={Gaussier, Eric and Glesser, David and Reis, Valentin and Trystram, Denis},  
  booktitle={SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},   
  title={Improving backfilling by using machine learning to predict running times},   
  year={2015},  
  volume={},  
  number={},  
  pages={1-10},  
  doi={10.1145/2807591.2807646}
}

@inproceedings{rl-sched,
  author = {Zhang, Di and Dai, Dong and He, Youbiao and Bao, Forrest Sheng and Xie, Bing},
  title = {RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning},
  year = {2020},
  isbn = {9781728199986},
  publisher = {IEEE Press},
  abstract = {Today's high-performance computing (HPC) platforms are still dominated by batch jobs. Accordingly, effective batch job scheduling is crucial to obtain high system efficiency. Existing HPC batch job schedulers typically leverage heuristic priority functions to prioritize and schedule jobs. But, once configured and deployed by the experts, such priority functions can hardly adapt to the changes of job loads, optimization goals, or system settings, potentially leading to degraded system efficiency when changes occur. To address this fundamental issue, we present RLScheduler, an automated HPC batch job scheduler built on reinforcement learning. RLScheduler relies on minimal manual interventions or expert knowledge, but can learn high-quality scheduling policies via its own continuous 'trial and error'. We introduce a new kernel-based neural network structure and trajectory filtering mechanism in RLScheduler to improve and stabilize the learning process. Through extensive evaluations, we confirm that RLScheduler can learn high-quality scheduling policies towards various workloads and various optimization goals with relatively low computation cost. Moreover, we show that the learned models perform stably even when applied to unseen workloads, making them practical for production use.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {31},
  numpages = {15},
  location = {Atlanta, Georgia},
  series = {SC '20}
}

@misc{lsf_documentation, 
  title={IBM Spectrum LSF Session Scheduler}, 
  url={https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsf-session-scheduler}, 
  journal={Platform LSF Session Scheduler}, 
  year={2021}
}

@misc{slurm_documentation, 
  title={Slurm Workload Manager}, 
  url={https://slurm.schedmd.com/documentation.html}, 
  journal={Slurm Workload Manager - Documentation}, 
  year={2020}
} 

@INPROCEEDINGS{flux,  
  author={Ahn, Dong H. and Bass, Ned and Chu, Albert and Garlick, Jim and Grondona, Mark and Herbein, Stephen and Koning, Joseph and Patki, Tapasya and Scogland, Thomas R. W. and Springmeyer, Becky and Taufer, Michela},  
  booktitle={2018 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)},   
  title={Flux: Overcoming Scheduling Challenges for Exascale Workflows},   
  year={2018},  
  volume={},  
  number={},  
  pages={10-19},  
  doi={10.1109/WORKS.2018.00007}
}

@inproceedings{tanash-pearc19,
  author = {Tanash, Mohammed and Dunn, Brandon and Andresen, Daniel and Hsu, William and Yang, Huichen and Okanlawon, Adedolapo},
  title = {Improving HPC System Performance by Predicting Job Resources via Supervised Machine Learning},
  year = {2019},
  isbn = {9781450372275},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3332186.3333041},
  doi = {10.1145/3332186.3333041},
  abstract = {High-Performance Computing (HPC) systems are resources utilized for data capture, sharing, and analysis. The majority of our HPC users come from other disciplines than Computer Science. HPC users including computer scientists have difficulties and do not feel proficient enough to decide the required amount of resources for their submitted jobs on the cluster. Consequently, users are encouraged to over-estimate resources for their submitted jobs, so their jobs will not be killing due insufficient resources. This process will waste and devour HPC resources; hence, this will lead to inefficient cluster utilization. We created a supervised machine learning model and integrated it into the Slurm resource manager simulator to predict the amount of required memory resources (Memory) and the required amount of time to run the computation. Our model involves using different machine learning algorithms. Our goal is to integrate and test the proposed supervised machine learning model on Slurm. We used over 10000 tasks selected from our HPC log files to evaluate the performance and the accuracy of our integrated model. The purpose of our work is to increase the performance of the Slurm by predicting the amount of require jobs memory resources and the time required for each particular job in order to improve the utilization of the HPC system using our integrated supervised machine learning model.Our results indicate that for larger jobs our model helps dramatically reduce computational turnaround time (from five days to ten hours for large jobs), substantially increased utilization of the HPC system, and decreased the average waiting time for the submitted jobs.},
  booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
  articleno = {69},
  numpages = {8},
  keywords = {Performance, Slurm, HPC, Scheduling, User Modeling, Supervised Machine Learning},
  location = {Chicago, IL, USA},
  series = {PEARC '19}
}

@inproceedings{li-ross19,
  author = {Li, Boyang and Chunduri, Sudheer and Harms, Kevin and Fan, Yuping and Lan, Zhiling},
  title = {The Effect of System Utilization on Application Performance Variability},
  year = {2019},
  isbn = {9781450367554},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3322789.3328743},
  doi = {10.1145/3322789.3328743},
  abstract = {Application performance variability caused by network contention is a major issue on dragonfly based systems. This work-in-progress study makes two contributions. First, we analyze real workload logs and conduct application experiments on the production system Theta at Argonne to evaluate application performance variability. We find a strong correlation between system utilization and performance variability where a high system utilization (e.g., above 95%) can cause up to 21% degradation in application performance. Next, driven by this key finding, we investigate a scheduling policy to mitigate workload interference by leveraging the fact that production systems often exhibit diurnal utilization behavior and not all users are in a hurry for job completion. Preliminary results show that this scheduling design is capable of improving system productivity (measured by scheduling makespan) as well as improving user-level scheduling metrics such as user wait time and job slowdown.},
  booktitle = {Proceedings of the 9th International Workshop on Runtime and Operating Systems for Supercomputers},
  pages = {11–18},
  numpages = {8},
  keywords = {job scheduling, dragonfly network, system utilization, performance variability, application experiments},
  location = {Phoenix, AZ, USA},
  series = {ROSS '19}
}

@inproceedings{luo-ics2012,
  author = {Luo, Miao and Panda, Dhabaleswar K. and Ibrahim, Khaled Z. and Iancu, Costin},
  title = {Congestion Avoidance on Manycore High Performance Computing Systems},
  year = {2012},
  isbn = {9781450313162},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2304576.2304594},
  doi = {10.1145/2304576.2304594},
  abstract = {Efficient communication is a requirement for application scalability on High Performance Computing systems. In this paper we argue for incorporating proactive congestion avoidance mechanisms into the design of communication layers on manycore systems. This is in contrast with the status quo which employs a reactive approach, emph{e.g.} congestion control mechanisms are activated only when resources have been exhausted. We present a core stateless optimization approach based on open loop end-point throttling, implemented for two UPC runtimes (Cray and Berkeley UPC) and validated on InfiniBand and the Cray Gemini networks. Microbenchmark results indicate that throttling the number of messages in flight per core can provide up to 4X performance improvements, while throttling the number of active cores per node can provide additional 40% and 6X performance improvement for UPC and MPI respectively. We evaluate inline (each task makes independent decisions) and proxy (server) congestion avoidance designs. Our runtime provides both performance and performance portability. We improve all-to-all collective performance by up to 4X and provide better performance than vendor provided MPI and UPC implementations. We also demonstrate performance improvements of up to 60% in application settings. Overall, our results indicate that modern systems accommodate only a surprisingly small number of messages in flight per node. As Exascale projections indicate that future systems are likely to contain hundreds to thousands of cores per node, we believe that their networks will be underprovisioned. In this situation, proactive congestion avoidance might become mandatory for performance improvement and portability.},
  booktitle = {Proceedings of the 26th ACM International Conference on Supercomputing},
  pages = {121–132},
  numpages = {12},
  keywords = {cray, high performance computing, infiniband, management, manycore, avoidance, multicore, congestion},
  location = {San Servolo Island, Venice, Italy},
  series = {ICS '12}
}

@article{patke-arxiv2020,
  author    = {Archit Patke and
               Saurabh Jha and
               Haoran Qiu and
               Jim M. Brandt and
               Ann C. Gentile and
               Joe Greenseid and
               Zbigniew Kalbarczyk and
               Ravishankar K. Iyer},
  title     = {Application-aware Congestion Mitigation forHigh-Performance Computing
               Systems},
  journal   = {CoRR},
  volume    = {abs/2012.07755},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.07755},
  archivePrefix = {arXiv},
  eprint    = {2012.07755},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-07755.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{easy-backfill-lifka,
  author = {Lifka, David A.},
  title = {The ANL/IBM SP Scheduling System},
  year = {1995},
  isbn = {3540601538},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  booktitle = {Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing},
  pages = {295–303},
  numpages = {9},
  series = {IPPS '95}
}

@inproceedings{easy-backfill,
  author = {Skovira, Joseph and Chan, Waiman and Zhou, Honbo and Lifka, David A.},
  title = {The EASY - LoadLeveler API Project},
  year = {1996},
  isbn = {3540618643},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  booktitle = {Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing},
  pages = {41–47},
  numpages = {7},
  series = {IPPS '96}
}

@software{kafka,
  author = {{Apache Software Foundation}},
  title = {Kafka},
  url = {https://kafka.apache.org/}
}

@software{spark,
  author = {{Apache Software Foundation}},
  title = {Spark},
  url = {https://spark.apache.org/}
}

@article{amg-proxy-app,
  title = {BoomerAMG: A parallel algebraic multigrid solver and preconditioner},
  journal = {Applied Numerical Mathematics},
  volume = {41},
  number = {1},
  pages = {155-177},
  year = {2002},
  note = {Developments and Trends in Iterative Methods for Large Systems of Equations - in memorium Rudiger Weiss},
  issn = {0168-9274},
  doi = {https://doi.org/10.1016/S0168-9274(01)00115-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0168927401001155},
  author = {Van Emden Henson and Ulrike Meier Yang},
  keywords = {Algebraic multigrid, Parallel computing},
  abstract = {Driven by the need to solve linear systems arising from problems posed on extremely large, unstructured grids, there has been a recent resurgence of interest in algebraic multigrid (AMG). AMG is attractive in that it holds out the possibility of multigrid-like performance on unstructured grids. The sheer size of many modern physics and simulation problems has led to the development of massively parallel computers, and has sparked much research into developing algorithms for them. Parallelizing AMG is a difficult task, however. While much of the AMG method parallelizes readily, the process of coarse-grid selection, in particular, is fundamentally sequential in nature. We have previously introduced a parallel algorithm [A.J. Cleary, R.D. Falgout, V.E. Henson, J.E. Jones, in: Proceedings of the Fifth International Symposium on Solving Irregularly Structured Problems in Parallel, Springer, New York, 1998] for the selection of coarse-grid points, based on modifications of certain parallel independent set algorithms and the application of heuristics designed to insure the quality of the coarse grids, and shown results from a prototype serial version of the algorithm. In this paper we describe an implementation of a parallel AMG code, using the algorithm of A.J. Cleary, R.D. Falgout, V.E. Henson, J.E. Jones [in: Proceedings of the Fifth International Symposium on Solving Irregularly Structured Problems in Parallel, Springer, New York, 1998] as well as other approaches to parallelizing the coarse-grid selection. We consider three basic coarsening schemes and certain modifications to the basic schemes, designed to address specific performance issues. We present numerical results for a broad range of problem sizes and descriptions, and draw conclusions regarding the efficacy of the method. Finally, we indicate the current directions of the research.}
}

@article{laghos-proxy-app,
  author = {Dobrev, Veselin A. and Kolev, Tzanio V. and Rieben, Robert N.},
  title = {High-Order Curvilinear Finite Element Methods for Lagrangian Hydrodynamics},
  journal = {SIAM Journal on Scientific Computing},
  volume = {34},
  number = {5},
  pages = {B606-B641},
  year = {2012},
  doi = {10.1137/120864672},
  URL = { 
          https://doi.org/10.1137/120864672
  },
  eprint = { 
          https://doi.org/10.1137/120864672 
  }
}

@article{aksar-sc19,
  title = {A Machine Learning Approach to Understanding HPC Application Performance Variation.},
  author = {Schwaller, Benjamin and Aksar, Burak and Aaziz, Omar Raad and Ates, Emre and Brandt, James M. and Coskun, Ayse and Egele, Manuel and Leung, Vitus J.},
  abstractNote = {Abstract not provided.},
  doi = {},
  url = {https://www.osti.gov/biblio/1642784}, journal = {},
  place = {United States},
  year = {2019},
  month = {10}
}

@misc{swfft,
  author = {Adrian Pope et al},
  title = {Swfft},
  year = {2017},
  publisher = {CELS ANL},
  journal = {Git repository},
  howpublished = {https://git.cels.anl.gov/hacc/SWFFT}
}

@misc{pennant,
  author = {Charles R. Ferenbaugh},
  title = {Pennant},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/lanl/PENNANT}
}

@misc{sw4lite,
  title = {sw4lite},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/geodynamics/sw4lite}
}

@misc{oms,
  title = {OpenSM Monitoring Service},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/meier/opensm-client-server}
}

@INPROCEEDINGS{caliper,  
  author={Boehme, David and Gamblin, Todd and Beckingsale, David and Bremer, Peer-Timo and Gimenez, Alfredo and LeGendre, Matthew and Pearce, Olga and Schulz, Martin},  
  booktitle={SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},   
  title={Caliper: Performance Introspection for HPC Software Stacks},   
  year={2016},  
  volume={},  
  number={},  
  pages={550-560},  
  doi={10.1109/SC.2016.46}
}

@inproceedings{klusavcek2018evaluating,
  title={Evaluating the impact of soft walltimes on job scheduling performance},
  author={Klus{\'a}{\v{c}}ek, Dalibor and Chlumsk{\`y}, V{\'a}clav},
  booktitle={Workshop on Job Scheduling Strategies for Parallel Processing},
  pages={15--38},
  year={2018}
}

@INPROCEEDINGS{aaziz-cluster2018,  
  author={Aaziz, Omar and Cook, Jonathan and Tanash, Mohammed},  
  booktitle={2018 IEEE International Conference on Cluster Computing (CLUSTER)},   
  title={Modeling Expected Application Runtime for Characterizing and Assessing Job Performance},   
  year={2018},  
  volume={},  
  number={},  
  pages={543-551},  
  doi={10.1109/CLUSTER.2018.00070}
}

@misc{gotcha,
  title         = "Gotcha Developer’s Guide",
  url           = {http://doi.acm.org/10.1145/1219092.1219093},
}

@misc{darshan,
  title         = "Darshan",
  url           = {https://www.mcs.anl.gov/research/projects/darshan},
}

@misc{got,
  author        = "Linux Foundation",
  title        = "Dynamic Linking",
  url           = {https://refspecs.linuxfoundation.org/ELF/zSeries/lzsabi0_zSeries/x2251.html},
}

@misc{unifyfs,
  title = {UnifyFS: A Distributed Burst Buffer File System - 0.1.0},
  author = {Moody, Adam and Sikich, Danielle and Bass, Ned and Brim, Michael J. and Stanavige, Cameron and Sim, Hyogi and Moore, Joseph and Hutter, Tony and Boehm, Swen and Mohror, Kathryn and Ivanov, Dmitry and Wang, Teng and Steffen, Craig P. and USDOE National Nuclear Security Administration},
  url = {https://www.osti.gov//servlets/purl/1408515},
  doi = {10.11578/dc.20200519.19},
  url = {https://www.osti.gov/biblio/1408515},
  year = {2017},
  month = {10},
}

@INPROCEEDINGS{recorder,
  author={Wang, Chen and Sun, Jinghan and Snir, Marc and Mohror, Kathryn and Gonsiorowski, Elsa},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Recorder 2.0: Efficient Parallel I/O Tracing and Analysis}, 
  year={2020},
  pages={1-8},
  doi={10.1109/IPDPSW50202.2020.00176}
}

@article{hpctoolkit,
  title={HPCToolkit: Tools for performance analysis of optimized parallel programs},
  author={Adhianto, Laksono and Banerjee, Sinchan and Fagan, Mike and Krentel, Mark and Marin, Gabriel and Mellor-Crummey, John and Tallent, Nathan R},
  journal={Concurrency and Computation: Practice and Experience},
  volume={22},
  number={6},
  pages={685--701},
  year={2010},
  publisher={Wiley Online Library}
}

@article{tallent2009binary,
  title={Binary analysis for measurement and attribution of program performance},
  author={Tallent, Nathan R and Mellor-Crummey, John M and Fagan, Michael W},
  journal={ACM Sigplan Notices},
  volume={44},
  number={6},
  pages={441--452},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{browne2000portable,
  title={A portable programming interface for performance evaluation on modern processors},
  author={Browne, Shirley and Dongarra, Jack and Garner, Nathan and Ho, George and Mucci, Philip},
  journal={The international journal of high performance computing applications},
  volume={14},
  number={3},
  pages={189--204},
  year={2000},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@InProceedings{scorep,
  author="Kn{\"u}pfer, Andreas and R{\"o}ssel, Christian and Mey, Dieter an and Biersdorff, Scott and Diethelm, Kai and Eschweiler, Dominic and Geimer, Markus and Gerndt, Michael and Lorenz, Daniel and Malony, Allen and Nagel, Wolfgang E. and Oleynik, Yury and Philippen, Peter and Saviankou, Pavel and Schmidl, Dirk and Shende, Sameer and Tsch{\"u}ter, Ronny and Wagner, Michael and Wesarg, Bert and Wolf, Felix", editor="Brunst, Holger and M{\"u}ller, Matthias S. and Nagel, Wolfgang E. and Resch, Michael M.",
  title="Score-P: A Joint Performance Measurement Run-Time Infrastructure for Periscope,Scalasca, TAU, and Vampir",
  booktitle="Tools for High Performance Computing 2011",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="79--91",
  abstract="This paper gives an overview about the Score-P performance measurement infrastructure which is being jointly developed by leading HPC performance tools groups. It motivates the advantages of the joint undertaking from both the developer and the user perspectives, and presents the design and components of the newly developed Score-P performance measurement infrastructure. Furthermore, it contains first evaluation results in comparison with existing performance tools and presents an outlook to the long-term cooperative development of the new system.",
  isbn="978-3-642-31476-6"
}

@inproceedings{timemory,
  title={TiMemory: modular performance analysis for HPC},
  author={Madsen, Jonathan R and Awan, Muaaz G and Brunie, Hugo and Deslippe, Jack and Gayatri, Rahul and Oliker, Leonid and Wang, Yunsong and Yang, Charlene and Williams, Samuel},
  booktitle={International Conference on High Performance Computing},
  pages={434--452},
  year={2020},
  organization={Springer}
}

@inproceedings{leko2008practical,
  title={Practical experiences with modern parallel performance analysis tools: an evaluation},
  author={Leko, Adam and Sherburne, Hans and Su, H and Golden, Bryan and George, Alan D},
  booktitle={Parallel and Distributed Processing, IPDPS 2008 IEEE Symposium},
  pages={14--18},
  year={2008}
}

@techreport{LULESH2:changes,
  author = {Ian Karlin and Jeff Keasler and Rob Neely},
  title = {LULESH 2.0 Updates and Changes},number = {LLNL-TR-641973},
  location = {Livermore, CA},
  pages = {1-9},
  year = {2013},
  month = {August}
}

@techreport{nethercote2004dynamic,
  title={Dynamic binary analysis and instrumentation},
  author={Nethercote, Nicholas},
  year={2004},
  institution={University of Cambridge, Computer Laboratory}
}

@inproceedings{froyd2005low,
  title={Low-overhead call path profiling of unmodified, optimized code},
  author={Froyd, Nathan and Mellor-Crummey, John and Fowler, Rob},
  booktitle={Proceedings of the 19th annual international conference on Supercomputing},
  pages={81--90},
  year={2005}
}

@incollection{knupfer2008vampir,
  title={The vampir performance analysis tool-set},
  author={Kn{\"u}pfer, Andreas and Brunst, Holger and Doleschal, Jens and Jurenz, Matthias and Lieber, Matthias and Mickler, Holger and M{\"u}ller, Matthias S and Nagel, Wolfgang E},
  booktitle={Tools for high performance computing},
  pages={139--155},
  year={2008},
  publisher={Springer}
}

@inproceedings{pdt:lindlan2000tool,
  title={A tool framework for static and dynamic analysis of object-oriented software with templates},
  author={Lindlan, Kathleen A and Cuny, Janice and Malony, Allen D and Shende, Sameer and Mohr, Bernd and Rivenburgh, Reid and Rasmussen, Craig},
  booktitle={SC'00: Proceedings of the 2000 ACM/IEEE Conference on Supercomputing},
  pages={49--49},
  year={2000},
  organization={IEEE}
}

@article{liu2014tool,
  title={A tool to analyze the performance of multithreaded programs on NUMA architectures},
  author={Liu, Xu and Mellor-Crummey, John},
  journal={ACM Sigplan Notices},
  volume={49},
  number={8},
  pages={259--272},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@inproceedings{malony2014general,
  title={General hybrid parallel profiling},
  author={Malony, Allen D and Huck, Kevin A},
  booktitle={2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing},
  pages={204--212},
  year={2014},
  organization={IEEE}
}

@article{shende2003integration,
  title={Integration and application of TAU in parallel Java environments},
  author={Shende, Sameer and Malony, Allen D},
  journal={Concurrency and Computation: Practice and Experience},
  volume={15},
  number={3-5},
  pages={501--519},
  year={2003},
  publisher={Wiley Online Library}
}

@inproceedings{nataraj2007tauoversupermon,
  title={Tauoversupermon: Low-overhead online parallel performance monitoring},
  author={Nataraj, Aroon and Sottile, Matthew and Morris, Alan and Malony, Allen D and Shende, Sameer},
  booktitle={European Conference on Parallel Processing},
  pages={85--96},
  year={2007},
  organization={Springer}
}

@article{allamanis2018survey,
  title={A survey of machine learning for big code and naturalness},
  author={Allamanis, Miltiadis and Barr, Earl T and Devanbu, Premkumar and Sutton, Charles},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={81},
  year={2018},
  publisher={ACM}
}

@inproceedings{Nitin2021DIRECTA,
  title={DIRECT : A Transformer-based Model for Decompiled Identifier Renaming},
  author={Vikram Nitin and Anthony Saieva and Baishakhi Ray and Gail E. Kaiser},
  booktitle={NLP4PROG},
  year={2021}
}

@misc{codex-copilot-all-author,
	Author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
	Title = {Evaluating Large Language Models Trained on Code},
	Year = {2021},
	Eprint = {arXiv:2107.03374},
}

@misc{codex-copilot-short-author,
	Author = {Mark Chen and et al},
	Title = {Evaluating Large Language Models Trained on Code},
	Year = {2021},
	Eprint = {arXiv:2107.03374},
}


@InProceedings{pmlr-v139-cummins21a,
  title = 	 {ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations},
  author =       {Cummins, Chris and Fisches, Zacharias V. and Ben-Nun, Tal and Hoefler, Torsten and O'Boyle, Michael F P and Leather, Hugh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2244--2253},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/cummins21a/cummins21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/cummins21a.html},
}

@inproceedings{neural-bug-finding_gupta_NEURIPS2019,
	author = {Gupta, Rahul and Kanade, Aditya and Shevade, Shirish},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Neural Attribution for Semantic Bug-Localization in Student Programs},
	url = {https://proceedings.neurips.cc/paper/2019/file/f29a179746902e331572c483c45e5086-Paper.pdf},
	volume = {32},
	year = {2019}
}

@Article{android-malware-detection-ml-survey-2021,
	AUTHOR = {Senanayake, Janaka and Kalutarage, Harsha and Al-Kadri, Mhd Omar},
	TITLE = {Android Mobile Malware Detection Using Machine Learning: A Systematic Review},
	JOURNAL = {Electronics},
	VOLUME = {10},
	YEAR = {2021},
	NUMBER = {13},
	ARTICLE-NUMBER = {1606},
	URL = {https://www.mdpi.com/2079-9292/10/13/1606},
	ISSN = {2079-9292},
	DOI = {10.3390/electronics10131606}
}

@InProceedings{huang2021cosqa,
	author = {Huang, Junjie and Tang, Duyu and Shou, Linjun and Gong (YIMING), Ming and Xu, Ke and Jiang, Daxin and Zhou, Ming and Duan, Nan},
	title = {CoSQA: 20,000+ Web Queries for Code Search and Question Answering},
	booktitle = {ACL-IJCNLP 2021},
	year = {2021},
	month = {May},
	url = {https://www.microsoft.com/en-us/research/publication/cosqa-20-000-web-queries-for-code-search-and-question-answering/},
}

@INPROCEEDINGS{code-queries-nlp-yan-icsaer-2020,  author={Yan, Shuhan and Yu, Hang and Chen, Yuting and Shen, Beijun and Jiang, Lingxiao},  booktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},   title={Are the Code Snippets What We Are Searching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries},   year={2020},  volume={},  number={},  pages={344-354},  doi={10.1109/SANER48275.2020.9054840}}

@article{Mir2021Type4PyDS,
  title={Type4Py: Deep Similarity Learning-Based Type Inference for Python},
  author={Amir M. Mir and Evaldas Latoskinas and Sebastian Proksch and Georgios Gousios},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.04470}
}

@misc{ast-tree-lstm-tanzima-arxiv-2021,
	Author = {Nathan Pinnow and Tarek Ramadan and Tanzima Z. Islam and Chase Phelps and Jayaraman J. Thiagarajan},
	Title = {Comparative Code Structure Analysis using Deep Learning for Performance Prediction},
	Year = {2021},
	Eprint = {arXiv:2102.07660},
}

@article{Wu2021ProtoTransformerAM,
  title={ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback},
  author={Mike Wu and Noah D. Goodman and Chris Piech and Chelsea Finn},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.14035}
}

@article{Zhu2021ASE,
  title={A syntax-guided edit decoder for neural program repair},
  author={Qihao Zhu and Zeyu Sun and Yuan-an Xiao and Wenjie Zhang and Kang Yuan and Yingfei Xiong and Lu Zhang},
  journal={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  year={2021}
}

@article{Bhatia2018NeuroSymbolicPC,
  title={Neuro-Symbolic Program Corrector for Introductory Programming Assignments},
  author={Sahil Bhatia and Pushmeet Kohli and Rishabh Singh},
  journal={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  year={2018},
  pages={60-70}
}
